\chapter{Die $\mathcal{O}$-Notation} 
In diesem Kapitel  stellen wir
die $O$-Notation vor.  Diese beiden Begriffe benötigen wir, um die Laufzeit von
Algorithmen analysieren zu können.  Die Algorithmen selber stehen in diesem Kapitel noch
im Hintergrund.

\section{Motivation}
Wollen wir die Komplexität eines Algorithmus abschätzen, so wäre ein mögliches Vorgehen
wie folgt: Wir kodieren den Algorithmus in einer Programmiersprache und berechnen,
wieviele Additionen, Multiplikationen, Zuweisungungen, und andere elementare Operationen
bei einer gegebenen Eingabe von dem Programm ausgeführt werden. Anschließend schlagen wir
im Prozessor-Handbuch nach, wieviel Zeit die einzelnen Operationen in Anspruch nehmen und
errechnen daraus die Gesamtlaufzeit des Programms.\footnote{
Da die heute verfügbaren Prozessoren fast alle mit \emph{Pipelining} arbeiten, werden oft
mehrere Befehle gleichzeitig abgearbeitet. Da gleichzeitig auch das Verhalten des Caches
eine wichtige Rolle spielt, ist die genaue Berechnung der Rechenzeit faktisch unmöglich.}
Dieses Vorgehen ist aber in zweifacher Hinsicht problematisch:
\begin{enumerate}
\item Das Verfahren ist sehr kompliziert.
\item Würden wir den selben Algorithmus anschließend in einer anderen Programmier-Sprache
      kodieren, oder aber das Programm auf einem anderen Rechner laufen lassen, so wäre
      unsere Rechnung wertlos und wir müssten sie wiederholen.
\end{enumerate}
Der letzte Punkt zeigt, dass das Verfahren dem Begriff des Algorithmus, der ja eine
Abstraktion des Programm-Begriffs ist, nicht gerecht wird.  Ähnlich wie der Begriff des
Algorithmus von bestimmten Details einer Implementierung abstrahiert brauchen wir zur
Erfassung der rechenzeitlichen Komplexität eines Algorithmus einen Begriff, der von
bestimmten Details der Funktion, die die Rechenzeit für ein gegebenes Programm berechnet,
abstrahiert.  Wir haben drei Forderungen an den zu findenden  Begriff.
\begin{itemize}
\item Der Begriff soll von konstanten Faktoren abstrahieren.
\item Der Begriff soll von \emph{unwesentlichen Termen} abstrahieren.

      Nehmen wir an, wir hätten ein Programm, dass zwei $n \times n$ Matrizen
      multipliziert und wir hätten für die Rechenzeit $T(n)$ dieses Programms in Abhängigkeit von
      $n$ die Funktion \\[0.1cm]
      \hspace*{1.3cm} $T(n) = 3 \cdot n^3 + 2 \cdot n^2 + 7$ \\[0.1cm]
      gefunden.  Dann nimmt der \emph{proportionale Anteil} des Terms $2\cdot n^2 + 7$ an der
      gesamten Rechenzeit mit wachsendem $n$ immer mehr ab.  Zur Verdeutlichung haben wir
      in einer Tabelle die Werte des proportionalen Anteils für 
      $n = 1,\; 10,\; 100,\; 1000,\, 10\,000$ aufgelistet: \\[0.3cm]
      \hspace*{1.3cm} 
      \begin{tabular}{|r|r|}
        \hline
        $n$  & \rule{0pt}{16pt} $\bruch{2 \cdot n^2 + 7}{3 \cdot n^3 + 2 \cdot n^2 + 7}$ \\[0.3cm]
        \hline
        \hline
        1       &  0.75000000000000  \\
        10      &  0.06454630495800  \\
        100     &  0.00662481908150  \\
        1000    &  0.00066622484855  \\
        10\,000 &  6.6662224852\,e\,-05  \\
       \hline
      \end{tabular}
\item Der Begriff soll das \emph{Wachstum} der Rechenzeit abhängig von \emph{Wachstum} der
      Eingaben erfassen. Welchen genauen Wert die Rechenzeit für kleine Werte der Eingaben
      hat, spielt nur eine untergeordnete Rolle, denn für kleine Werte der Eingaben 
      wird auch die Rechenzeit nur klein sein.
\end{itemize}
Wir bezeichnen die Menge der positiven reellen Zahlen mit $\R_+$ \\[0.1cm]
\hspace*{1.3cm} $\R_+ := \{ x \in \R \mid x > 0 \}$. \\[0.1cm]
Wir bezeichnen die Menge aller Funktionen von $\N$ nach
 $\R_+$ mit $\R_+^{\;\N}$, es gilt also: \\[0.1cm]
\hspace*{1.3cm} 
$\R_+^{\;\N} = \bigl\{ f \mid \mbox{$f$ ist Funktion der Form $f: \N \rightarrow \R_+$} \}$.

\begin{Definition}[$\Oh(f)$] {\em
  Es sei eine Funktion $f\in \R_+^\N$ 
  gegeben.   Dann definieren wir die Menge der Funktionen, die asymptotisch
  das gleiche Wachstumsverhalten haben wie die Funktion $f$, wie folgt:
  \\[0.1cm]
  \hspace*{1.3cm} 
  $ \Oh(f) \;:=\; \left\{ g \in \R_+^{\;\N} \mid \exists k \in \N \colon 
    \bigl(\exists c \in \R\colon \forall n \in \N \colon n \geq k \rightarrow g(n) \leq c \cdot f(n)\bigr) \right\}$.
  \hspace*{\fill} $\Box$
}
\end{Definition}
Was sagt die obige Definition aus? Zunächst kommt es auf kleine Werte des Arguments $n$
nicht an, denn die obige Formel sagt ja, dass $g(n) \leq c \cdot f(n)$ nur für die $n$ gelten
muss, für die  $n \geq k$ ist.  Außerdem kommt es auf Proportionalitäts-Konstanten nicht
an, denn $g(n)$ muss ja nur kleinergleich $c \cdot f(n)$ sein und die Konstante $c$ können wir
beliebig wählen.  Um den Begriff zu verdeutlichen, geben wir einige Beispiele.
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}: Es gilt \\[0.1cm]
\hspace*{1.3cm} $3 \cdot n^3 + 2 \cdot n^2 + 7 \in \Oh(n^3)$. \\[0.1cm]
\textbf{Beweis}: Wir müssen eine Konstante $c$ und eine Konstante $k$ angeben, so dass für
alle $n\in \N$ mit $n \geq k$ die Ungleichung
\\[0.1cm]
\hspace*{1.3cm} 
$3 \cdot n^3 + 2 \cdot n^2 + 7 \leq c \cdot n^3$
\\[0.1cm]
gilt.  Wir setzen  $k := 1$ und $c := 12$. Dann können wir die Ungleichung 
\begin{equation}
  \label{eq:u1}
  1\leq n  
\end{equation}
voraussetzen und müssen zeigen, dass daraus 
\begin{equation}
  \label{eq:u2}
  3 \cdot n^3 + 2 \cdot n^2 + 7 \leq 12 \cdot n^3  
\end{equation}
folgt. Erheben wir beide Seiten der  Ungleichung (\ref{eq:u1}) in die dritte Potenz, so sehen wir,
dass 
\begin{equation}
  \label{eq:u3pre}
  1 \leq n^3  
\end{equation}
gilt.  Diese Ungleichung multiplizieren wir auf beiden Seiten mit $7$ und erhalten: 
\begin{equation}
  \label{eq:u3}
  7 \leq 7 \cdot n^3
\end{equation}
Multiplizieren wir die Ungleichung (\ref{eq:u1}) mit $2\cdot n^2$, so erhalten wir 
\begin{equation}
  \label{eq:u4}
  2 \cdot n^2 \leq 2 \cdot n^3  
\end{equation}
Schließlich gilt trivialerweise 
\begin{equation}
  \label{eq:u5}
  3 \cdot n^3 \leq 3 \cdot n^3
\end{equation}
Die Addition der Ungleichungen (\ref{eq:u3}), (\ref{eq:u4}) und (\ref{eq:u5}) liefert nun \\[0.1cm]
\hspace*{1.3cm} $3 \cdot n^3 + 2 \cdot n^2 + 7 \leq 12 \cdot n^3$ \\[0.1cm]
und das war zu zeigen. \hspace*{\fill} $\Box$
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}: Es gilt  $n \in \Oh(2^n)$. 
\vspace*{0.3cm}

\noindent
\textbf{Beweis}: Wir müssen eine Konstante $c$ und eine Konstante $k$ angeben, so dass für
alle $n \geq k$
\\[0.1cm]
\hspace*{1.3cm} $ n \leq c \cdot 2^n$ \\[0.1cm]
gilt.  Wir setzen $k := 0$ und $c := 1$.  Wir zeigen \\[0.1cm]
\hspace*{1.3cm} $n \leq 2^n$ \quad für alle $n \in \N$ \\[0.1cm]
durch vollständige Induktion über $n$.
\begin{enumerate}
\item \textbf{I.A.}: $n = 0$

      Es gilt $0 \leq 1 = 2^0$.
\item \textbf{I.S.}: $n \mapsto n + 1$

      Einerseits gilt nach Induktions-Voraussetzung \\[0.1cm]
      \hspace*{1.3cm} $n \leq 2^n$, \hspace*{\fill} $(1)$ \\[0.1cm]
      andererseits haben wir \\[0.1cm]
      \hspace*{1.3cm} $1 \leq 2^n$. \hspace*{\fill} $(2)$ \\[0.1cm]
      Addieren wir $(1)$ und $(2)$, so erhalten wir \\[0.1cm]
      \hspace*{1.3cm} $n+1 \leq 2^n + 2^n = 2^{n+1}$. \hspace*{\fill} $\Box$

      \textbf{Bemerkung}: Die Ungleichung $1 \leq 2^n$ hätten wir eigentlich ebenfalls
      durch Induktion  nachweisen müssen.
\end{enumerate}

\exercise
Zeigen Sie \\[0.1cm]
\hspace*{1.3cm} $n^2 \in \Oh(2^n)$.
\vspace*{0.3cm}

\noindent
Wir zeigen nun einige Eigenschaften der $\Oh$-Notation.

\begin{Satz}[Reflexivität]
{\em
  Für alle Funktionen $f\colon \N \rightarrow {\R_+}$ gilt \\[0.1cm]
  \hspace*{1.3cm} $f \in \Oh(f)$. 
}
\end{Satz}
\textbf{Beweis}: Wählen wir $k:=0$ und $c:=1$, so folgt die Behauptung sofort aus der
Ungleichung \\[0.1cm]
\hspace*{1.3cm} $\forall n \in \N\colon f(n) \leq f(n)$. \hspace*{\fill} $\Box$

\begin{Satz}[Abgeschlossenheit unter Multiplikation mit Konstanten] \hspace*{\fill} \\
{\em
  Es seien  $f,g\colon \N \rightarrow {\R_+}$
   und $d \in \R_+$.  Dann gilt \\[0.1cm]
  \hspace*{1.3cm} $g \in \Oh(f) \Rightarrow d \cdot g \in \Oh(f)$.
}
\end{Satz}
\textbf{Beweis}: Aus $g \in \Oh(f)$ folgt, dass es Konstanten $c'\in \R_+$, $k'\in \N$ gibt,
so dass \\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon \bigl(n \geq k'  \rightarrow g(n) \leq c' \cdot f(n)\bigr)$ \\[0.1cm]
gilt.  Multiplizieren wir die Ungleichung mit $d$, so haben wir \\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon\bigl( n \geq k'  \rightarrow d \cdot g(n) \leq d \cdot c' \cdot f(n)\bigr)$ \\[0.1cm]
Setzen wir nun $k:=k'$ und $c := d \cdot c'$, so folgt \\[0.1cm]
\hspace*{1.3cm} $\forall n \in \N \colon \bigl(n \geq k  \rightarrow d \cdot g(n) \leq c \cdot f(n)\bigr)$ 
\\[0.1cm]
und daraus folgt $d \cdot g \in \Oh(f)$. \hspace*{\fill} $\Box$.

\begin{Satz}[Abgeschlossenheit unter Addition]
{\em
  Es seien $f,g,h \colon \N \rightarrow {\R_+}$.  Dann gilt \\[0.1cm]
  \hspace*{1.3cm} $f \in \Oh(h) \wedge g \in \Oh(h) \,\rightarrow\, f + g \in \Oh(h)$.
}
\end{Satz}
\textbf{Beweis}: Aus den Voraussetzungen $f \in \Oh(h)$ und $g \in \Oh(h)$ folgt, dass es
Konstanten $k_1,k_2\in \N$ und $c_1,c_2\in \R$ gibt, so dass \\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon \bigl( n \geq k_1 \rightarrow f(n) \leq c_1 \cdot h(n)\bigr)$ 
\quad und\\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon\bigl( n \geq k_2 \rightarrow g(n) \leq c_2 \cdot h(n)\bigr)$
\\[0.1cm]
gilt.  Wir setzen $k := \max(k_1,k_2)$ und $c:= c_1 + c_2$.  Für $n \geq k$ gilt dann \\[0.1cm]
\hspace*{1.3cm} $f(n) \leq c_1 \cdot h(n)$ und $g(n) \leq c_2 \cdot h(n)$. \\[0.1cm]
Addieren wir diese beiden Gleichungen, dann haben wir für alle $n \geq k$ \\[0.1cm]
\hspace*{1.3cm} $f(n) + g(n) \leq (c_1 + c_2) \cdot h(n) = c \cdot h(n)$. \hspace*{\fill} $\Box$

\begin{Satz}[Transitivität]
{\em
  Es seien $f,g,h \colon \N \rightarrow {\R_+}$.  Dann gilt \\[0.1cm]
  \hspace*{1.3cm} $f \in \Oh(g) \wedge g \in \Oh(h) \,\rightarrow\, f \in \Oh(h)$.
}
\end{Satz}
\textbf{Beweis}: Aus $f \in \Oh(g)$ folgt, dass es $k_1 \in \N$ und $c_1 \in \R$ gibt, so dass\\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon\bigl( n \geq k_1 \rightarrow f(n) \leq c_1 \cdot g(n)\bigr)$ \\[0.1cm]
gilt und aus $g \in \Oh(h)$ folgt, dass es $k_2 \in \N$ und $c_2 \in \R$ gibt, so dass \\[0.1cm]
\hspace*{1.3cm} 
$\forall n \in \N \colon\bigl( n \geq k_2 \rightarrow g(n) \leq c_2 \cdot h(n)\bigr)$ \\[0.1cm]
gilt.  Wir definieren $k:= \max(k_1,k_2)$ und $c := c_1 \cdot c_2$.  Dann haben wir für alle
$n \geq k$:\\[0.1cm]
\hspace*{1.3cm} $f(n) \leq c_1\cdot g(n)$ und $g(n) \leq c_2 \cdot h(n)$. \\[0.1cm]
Die zweite dieser Ungleichungen multiplizieren wir mit $c_1$ und erhalten \\[0.1cm]
\hspace*{1.3cm} $f(n) \leq c_1\cdot g(n)$ und $c_1\cdot g(n) \leq c_1\cdot c_2 \cdot h(n)$. \\[0.1cm]
Daraus folgt aber sofort $f(n) \leq c \cdot h(n)$. \hspace*{\fill} $\Box$

\begin{Satz}[Grenzwert-Satz] \label{limit}
{\em
  Es seien $f,g \colon \N \rightarrow {\R_+}$.  Außerdem existiere der Grenzwert \\[0.1cm]
  \hspace*{1.3cm} $\lim\limits_{n \rightarrow \infty} \bruch{\,f(n)\,}{g(n)}$.  \\[0.1cm]
  Dann gilt $f \in \Oh(g)$. 
}
\end{Satz}
\textbf{Beweis}: Es sei \\[0.1cm]
\hspace*{1.3cm} $\lambda := \lim\limits_{n \rightarrow \infty} \bruch{f(n)}{g(n)}$.  \\[0.1cm]
Nach Definition des Grenzwertes gibt es dann eine Zahl $k \in \N$, so dass 
für alle $n\in \N$ mit $n \geq k$ die Ungleichung \\[0.1cm]
\hspace*{1.3cm} $\left| \bruch{f(n)}{g(n)} - \lambda \right| \leq 1$ \\[0.1cm]
gilt.  Multiplizieren wir diese Ungleichung mit $g(n)$, so erhalten wir \\[0.1cm]
\hspace*{1.3cm} $|f(n) - \lambda \cdot g(n)| \leq g(n)$. \\[0.1cm]
Daraus folgt wegen \\[0.1cm]
\hspace*{1.3cm} $f(n) \leq \bigl|f(n) - \lambda \cdot g(n)\bigr| + \lambda \cdot g(n)$ \\[0.1cm]
die Ungleichung \\[0.1cm]
\hspace*{1.3cm} $f(n) \leq g(n) + \lambda \cdot g(n) = (1 + \lambda) \cdot g(n)$. \\[0.1cm]
Definieren wir  $c := 1 +  \lambda$, 
so folgt für alle $n \geq k$ die Ungleichung $f(n) \leq c \cdot g(n)$. \hspace*{\fill} $\Box$
\vspace*{0.3cm}

\noindent
Wir zeigen die Nützlichkeit der obigen Sätze an Hand einiger Beispiele.
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}: Es sei $k \in \N$.  Dann gilt\\[0.1cm]
\hspace*{1.3cm} $n^k \in \Oh(n^{k+1})$.
\vspace*{0.3cm}

\noindent
\textbf{Beweis}: Es gilt \\[0.1cm]
\hspace*{1.3cm} 
$\lim\limits_{n \rightarrow \infty} \bruch{n^{k}}{n^{k+1}} = \lim\limits_{n \rightarrow   \infty} \bruch{1}{n} = 0$.
\\[0.1cm]
Die Behauptung folgt nun aus dem Grenzwert-Satz. \hspace*{\fill} $\Box$
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}: Es sei $k \in \N$ und $\lambda \in \R$ mit $\lambda > 1$.  Dann gilt\\[0.1cm]
\hspace*{1.3cm} $n^k \in \Oh(\lambda^n)$.
\vspace*{0.3cm}

\noindent
\textbf{Beweis}: Wir zeigen, dass 
\hspace*{1.3cm} 
\begin{equation}
  \label{eq:star}
  \lim\limits_{n \rightarrow \infty} \bruch{n^{k}}{\lambda^n} = 0  
\end{equation}
ist, denn dann folgt die Behauptung aus dem Grenzwert-Satz. 
Nach dem Satz von L'Hospital können wir den Grenzwert wie folgt berechnen \\[0.1cm]
\hspace*{1.3cm} 
$\displaystyle \lim\limits_{n \rightarrow \infty} \bruch{n^{k}}{\lambda^n} =
\lim\limits_{x \rightarrow \infty} \bruch{x^{k}}{\lambda^x} =
\lim\limits_{x \rightarrow \infty} \bruch{\;\frac{d\,x^{k}}{dx}\;}{\frac{d\,\lambda^x}{dx}}$
\\[0.1cm]
Die Ableitungen  können wir berechnen, es gilt: \\[0.1cm]
\hspace*{1.3cm}
 $\displaystyle \frac{d\,x^{k}}{dx} = k \cdot x^{k-1}$ \quad und \quad 
 $\displaystyle \frac{d\,\lambda^{x}}{dx} = \ln(\lambda) \cdot \lambda^x$. \\[0.1cm]
Berechnen wir die zweite Ableitung so sehen wir \\[0.1cm]
\hspace*{1.3cm}  
$\displaystyle \frac{d^{2}\,x^{k}}{dx^2} = k \cdot (k-1) \cdot x^{k-2}$ \quad und \quad 
 $\displaystyle \frac{d^2\,\lambda^{x}}{dx^2} = \ln(\lambda)^2 \cdot \lambda^x$. \\[0.1cm]
Für die $k$-te Ableitung gilt analog \\[0.1cm]
\hspace*{1.3cm} 
$\displaystyle \frac{d^{k}\,x^{k}}{dx^k} = k \cdot (k-1) \cdot \cdots \cdot 1 \cdot x^{0} = k!$ \quad und \quad 
 $\displaystyle \frac{d^k\,\lambda^{x}}{dx^k} = \ln(\lambda)^k \cdot \lambda^x$. \\[0.1cm]
Wenden wir also den Satz von L'Hospital zur Berechnung des Grenzwertes  $k$ mal an, so
sehen wir \\[0.1cm]
\hspace*{1.3cm} 
$\displaystyle 
\lim\limits_{x \rightarrow \infty} \bruch{x^{k}}{\lambda^x} =
\lim\limits_{x \rightarrow \infty} \bruch{\rule[-10pt]{0pt}{10pt}\;\frac{d\,x^{k}}{dx}\;}{\rule{0pt}{12pt}\frac{d\,\lambda^x}{dx}} =
\lim\limits_{x \rightarrow \infty} \bruch{\rule[-10pt]{0pt}{10pt}\;\frac{d^2\,x^{k}}{dx^2}\;}{\rule{0pt}{12pt}\frac{d^2\,\lambda^x}{dx^2}} =
\cdots = 
\lim\limits_{x \rightarrow \infty} \bruch{\rule[-10pt]{0pt}{10pt}\;\frac{d^k\,x^{k}}{dx^k}\;}{\rule{0pt}{12pt}\frac{d^k\,\lambda^x}{dx^k}} =
\lim\limits_{x \rightarrow \infty} \bruch{k!}{\ln(\lambda)^k \lambda^x} = 0$.

\hspace*{\fill} $\Box$
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}: Es gilt $\ln(n) \in \Oh(n)$.
\vspace*{0.3cm}

\noindent
\textbf{Beweis}: Wir benutzen Satz \ref{limit} und zeigen mit der Regel von L'Hospital,
dass \\[0.1cm]
\hspace*{1.3cm} 
$\lim\limits_{n \rightarrow \infty} \bruch{\;\ln(n)\;}{n} = 0$
\\[0.1cm]
ist.  Es gilt \\[0.1cm]
\hspace*{1.3cm} $\displaystyle \frac{d\, \ln(x)}{dx} = \frac{1}{x}$ 
\quad und \quad
 $\displaystyle \frac{d\, x}{dx} = 1$. \\[0.1cm]
Also haben wir \\[0.1cm]
\hspace*{1.3cm} 
$\displaystyle \lim\limits_{n \rightarrow \infty} \bruch{\;\ln(n)\;}{n} = 
\lim\limits_{x \rightarrow \infty} \bruch{\rule[-10pt]{0pt}{10pt}\;\frac{1}{x}\;}{1} = 
\lim\limits_{x \rightarrow \infty} \bruch{1}{x} = 0$. \hspace*{\fill} $\Box$
\vspace*{0.3cm}

\exercise
Zeigen Sie $\sqrt{n} \in \Oh(n)$.
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}:  Es gilt $2^n \in \Oh(3^n)$, aber $3^n \notin \Oh(2^n)$.
\vspace*{0.3cm}

\noindent
\textbf{Beweis}:  Zunächst haben wir \\[0.1cm]
\hspace*{1.3cm} 
$\displaystyle\lim\limits_{n \rightarrow \infty} \bruch{2^n}{3^n} = 
 \lim\limits_{n \rightarrow \infty} \left(\bruch{2}{3}\right)^n = 0$.
\\[0.1cm]
Den Beweis, dass $3^n \notin \Oh(2^n)$ ist, führen wir indirekt und nehmen an, dass 
$3^n \in \Oh(2^n)$ ist.  Dann muss es Konstanten $c$ und $k$ geben, so dass für alle $n
\geq k$ gilt \\[0.1cm]
\hspace*{1.3cm} $3^n \leq c \cdot 2^n$. \\[0.1cm]
Wir logarithmieren beide Seiten dieser Ungleichung und finden \\[0.1cm]
\[
\begin{array}{llcl}
                & \ln(3^n) & \leq & \ln(c \cdot 2^n) \\[0.1cm]
\leftrightarrow\quad &  n \cdot \ln(3) & \leq & \ln(c) + n \cdot \ln(2) \\[0.1cm]
\leftrightarrow &  n \cdot \bigl(\ln(3) - \ln(2)\bigr) & \leq & \ln(c)  \\[0.1cm]
\leftrightarrow &  n  & \leq & \bruch{\ln(c)}{\ln(3) - \ln(2)}  \\[0.1cm]
\end{array}
\]
Die letzte Ungleichung müsste nun für beliebig große natürliche Zahlen $n$ gelten und liefert
damit den gesuchten Widerspruch zu unserer Annahme.

\exercise
\begin{enumerate}
\item Es sei $b \geq 1$. Zeigen Sie $\log_{b}(n) \in \Oh(\ln(n))$.
\item $3 \cdot n^2 + 5 \cdot n + \sqrt{n} \in \Oh(n^2)$
\item $7 \cdot n + \bigl(\log_2(n)\bigr)^2 \in \Oh(n)$
\item $\sqrt{n} + \log_2(n) \in \Oh\left(\sqrt{n}\right)$
\item $n^n \in \mathcal{O}\bigl(2^{2^n}\bigr)$.

      Hinweis:  Die letzte Teilaufgabe ist schwer!
\end{enumerate}

\section{Fallstudie: Effiziente Berechnung der Potenz}
Wir  verdeutlichen die bisher eingeführten Begriffe an einem Beispiel.  Wir betrachten ein
Programm zur Berechnung der Potenz $m^n$ für natürliche Zahlen $m$ und $n$.
Abbildung \ref{fig:power-naive.stlx} zeigt ein naives Programm zur Berechnung von $m^n$.
Die diesem Programm zu Grunde liegende Idee ist es, die Berechnung von $m^n$ 
nach der Formel \\[0.1cm]
\hspace*{1.3cm} 
$m^n = \underbrace{m \cdot {\dots} \cdot m}_n$ \\[0.1cm]
durchzuführen.  

\begin{figure}[!h]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm
                ]
    power := procedure(m, n) {
        r := 1;
        for (i in {1 .. n}) {
            r := r * m;
        }
        return r;
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Naive Berechnung von $m^n$ für $m,n \in \N$.}
  \label{fig:power-naive.stlx}
\end{figure} 

Das Programm ist  offenbar korrekt.
Zur Berechnung  von $m^n$ werden für positive Exponenten $n$ insgesamt $n-1$ Multiplikationen durchgeführt.
Wir können $m^n$ aber wesentlich effizienter berechnen.  Die Grundidee erläutern wir an
der Berechnung von $m^4$.  Es gilt \\[0.1cm]
\hspace*{1.3cm} 
$m^4 = (m \cdot m) \cdot (m \cdot m)$.\\[0.1cm]
Wenn wir den Ausdruck $m\cdot m$ nur einmal berechnen, dann kommen wir bei der Berechnung von
$m^4$ nach der obigen Formel mit zwei Multiplikationen aus, während bei einem
naiven Vorgehen 3 Multiplikationen durchgeführt würden! Für die Berechnung von $m^8$
können wir folgende Formel verwenden: \\[0.1cm]
\hspace*{1.3cm} 
$m^8 = \bigl( (m \cdot m) \cdot (m \cdot m) \bigr) \cdot \bigl( (m \cdot m) \cdot (m \cdot m) \bigr)$. \\[0.1cm]
Berechnen wir den Term $(m \cdot m) \cdot (m \cdot m)$ nur einmal, so werden jetzt 3 Multiplikationen
benötigt um $m^8$ auszurechnen.  Ein naives Vorgehen würde 7 Multiplikationen benötigen.
Wir versuchen die oben an Beispielen erläuterte Idee in ein Programm umzusetzen.
Abbildung \ref{fig:power.stlx} zeigt das Ergebnis.  Es berechnet die Potenz $m^n$ nicht durch eine
naive $(n-1)$-malige Multiplikation sondern es verwendet das Paradigma \\[0.1cm]
\hspace*{1.3cm} \emph{Teile und Herrsche}. \quad (engl. \emph{divide and conquer})
\\[0.1cm]
Die Grundidee um den Term $m^n$ für $n \geq 1$ effizient zu berechnen,
lässt sich durch folgende Formel beschreiben: \\[0.1cm] 
\hspace*{1.3cm} 
$m^n = 
\left\{\begin{array}{ll}
m^{n/2} \cdot m^{n/2}      & \mbox{falls $n$ gerade ist};    \\
m^{n/2} \cdot m^{n/2} \cdot m  & \mbox{falls $n$ ungerade ist}.
\end{array}
\right.
$


\begin{figure}[!h]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm
                ]
    power := procedure(m, n) {
        if (n == 0) {
            return 1;
        }
        p := power(m, floor(n / 2));
        if (n % 2 == 0) {
            return p * p;
        } else {
            return p * p * m;
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Berechnung von $m^n$ für $m,n \in \N$.}
  \label{fig:power.stlx}
\end{figure} 

Da es keineswegs offensichtlich ist, dass das Programm in \ref{fig:power.stlx} 
tatsächlich die Potenz $m^n$ berechnet,  wollen wir dies nachweisen.  Wir benutzen dazu
die Methode der \emph{Wertverlaufs-Induktion} (engl. \emph{computational induction}).
Die Wertverlaufs-Induktion ist eine Induktion über die Anzahl der rekursiven Aufrufe.
Diese Methode bietet sich immer dann an, wenn die Korrektheit einer rekursiven Prozedur
nachzuweisen ist. Das Verfahren besteht aus zwei Schritten:
\begin{enumerate}
\item \emph{Induktions-Anfang}.

      Beim Induktions-Anfang weisen wir nach, dass die Prozedur in allen den Fällen korrekt arbeitet,
      in denen sie sich nicht selbst aufruft.  
\item \emph{Induktions-Schritt}

      Im Induktions-Schritt beweisen wir, dass die Prozedur auch in den Fällen korrekt
      arbeitet, in denen sie sich rekursiv aufruft.   Beim Beweis dieser Tatsache dürfen
      wir voraussetzen, dass die Prozedur bei jedem rekursiven Aufruf den korrekten Wert
      produziert. Diese Voraussetzung wird auch als \emph{Induktions-Voraussetzung} bezeichnet.
\end{enumerate}
Wir demonstrieren die Methode, indem wir durch Wertverlaufs-Induktion beweisen, dass 
gilt: 
\\[0.1cm]
\hspace*{1.3cm} $\mathtt{power}(m,n) \leadsto m^n$.
\begin{enumerate}
\item \textbf{Induktions-Anfang}.

      Die Methode ruft sich dann nicht rekursiv auf, wenn $n = 0$  gilt.  In diesem Fall
      haben wir \\[0.1cm]
      \hspace*{1.3cm} 
      $\mathtt{power}(m,0) \leadsto 1 =  m^0$.
\item \textbf{Induktions-Schritt}.

      Der rekursive Aufruf der Prozedur $\mathtt{power}$ hat die Form 
       $\mathtt{power}(m,n/2)$.  Also gilt nach Induktions-Voraussetzung \\[0.1cm]
       \hspace*{1.3cm} $\displaystyle \mathtt{power}(m,n/2) \leadsto m^{n/2}$. \\[0.1cm]
       Danach können in der weiteren Rechnung zwei Fälle auftreten.
       Wir führen daher eine Fallunterscheidung entsprechend der \texttt{if}-Abfrage in Zeile 6 durch:
      \begin{enumerate}
      \item $n \;\mathtt{\%}\; 2 = 0$, $n$ ist also gerade.

            Dann gibt es ein $k \in \N$ mit $n = 2 \cdot k$ und also ist $n/2 = k$.
            In diesem Fall gilt 
            \[ 
            \begin{array}{lcl}
            \mathtt{power}(m,n) & \leadsto & \mathtt{power}(m,k) \cdot \mathtt{power}(m,k) \\[0.1cm]
                                & \stackrel{I.V.}{\leadsto} & m^k \cdot m^k  \\[0.1cm]
                                & = & m^{2\cdot k} \\[0.1cm]
                                & = & m^{n}.
            \end{array}
            \]            
      \item $n \;\mathtt{\%}\; 2 = 1$, $n$ ist also ungerade.

            Dann gibt es ein $k \in \N$ mit $n = 2 \cdot k + 1$ und wieder ist $n/2 = k$.
            In diesem Fall gilt 
            \[ 
            \begin{array}{lcl}
            \mathtt{power}(m,n) & \leadsto & \mathtt{power}(m,k) \cdot \mathtt{power}(m,k) \cdot m  \\[0.1cm]
                                & \stackrel{I.V.}{\leadsto} & m^k \cdot m^k \cdot m  \\[0.1cm]
                                & = & m^{2\cdot k+1} \\[0.1cm]
                                & = & m^{n}.
            \end{array}
            \]
      \end{enumerate}
      Damit ist der Beweis der Korrektheit abgeschlossen. \hspace*{\fill} $\Box$
\end{enumerate}
Als nächstes wollen wir die Komplexität des obigen Programms untersuchen. Dazu berechnen
wir zunächst die Anzahl der Multiplikationen, die beim Aufruf $\mathtt{power}(m,n)$
durchgeführt werden.  Je nach dem, ob der Test in Zeile 6 negativ ausgeht oder nicht, gibt
es mehr oder weniger Multiplikationen.  Wir untersuchen zunächst den schlechtesten Fall
(engl. \emph{worst case}).  Der schlechteste Fall tritt dann ein, wenn es
ein $l\in \N$ gibt, so dass \\[0.1cm]
\hspace*{1.3cm} $n = 2^l - 1$ \\[0.1cm]
ist, denn dann gilt \\[0.1cm]
\hspace*{1.3cm} $n/2 = 2^{l-1} - 1$ \quad und \quad $n \,\texttt{\symbol{37}}\, 2 = 1$, \\[0.1cm]
was wir sofort durch die Probe
\\[0.2cm]
\hspace*{1.3cm}
$2 \cdot(n/2) + n \,\texttt{\symbol{37}}\, 2 = 2 \cdot (2^{l-1} - 1) + 1 = 2^l - 1 = n$
\\[0.2cm]
verifizieren.  Folglich ist, wenn $n$ die Form $2^l - 1$ hat, bei jedem rekursiven Aufruf
der Exponent $n$ ungerade. 
Wir nehmen also $n = 2^l - 1$ an und berechnen die Zahl $a_n$ der Multiplikationen, die beim
Aufruf von $\mathtt{power}(m,n)$ durchgeführt werden. \\[0.1cm]
Zunächst gilt $a_0 = 0$, denn wenn $n =0$  ist, wird keine Multiplikation durchgeführt.
Ansonsten haben wir in Zeile 9 zwei Multiplikationen, die zu den Multiplikationen, die beim
rekursiven Aufruf in Zeile $5$ anfallen, hinzu addiert werden müssen.  Damit erhalten wir
die folgende Rekurrenz-Gleichung: \\[0.1cm]
\hspace*{1.3cm} $a_n = a_{n/2} + 2$ \qquad für alle $n \in \left\{2^l - 1 \mid l \in \N\right\}$\quad mit $a_0 = 0$. \\[0.1cm]
Wir definieren $b_l := a_{2^l-1}$ und erhalten dann für die Folge $(b_l)_l$ die
Rekurrenz-Gleichung \\[0.1cm]
\hspace*{1.3cm} 
$b_l = a_{2^l-1} = a_{(2^l-1)/2} + 2 = a_{2^{l-1}-1} + 2 = b_{l-1} +2$ \qquad für alle $l\in\N$. \\[0.1cm]
Die Anfangs-Bedingung lautet $b_0 = a_{2^0-1} = a_0 = 0$.  
Offenbar lautet die Lösung der Rekurrenz-Gleichung \\[0.1cm]
\hspace*{1.3cm} $b_l = 2 \cdot l$ \qquad für alle $l \in \N$. \\[0.1cm] 
Diese Behauptung können Sie durch eine triviale Induktion verifizieren. 
Für die Folge $a_n$ haben wir dann: \\[0.1cm]
\hspace*{1.3cm} $a_{2^l-1} = 2 \cdot l$. \\[0.1cm]
Formen wir die Gleichung $n = 2^l - 1$ nach $l$ um, so erhalten wir $l =
\log_2(n+1)$. Setzen wir diesen Wert ein, so sehen wir \\[0.1cm]
\hspace*{1.3cm} $a_n = 2 \cdot \log_2(n+1) \in \Oh\bigl(\log_2(n)\bigr)$.
\vspace*{0.3cm}

Wir betrachten jetzt den günstigsten Fall, der bei der Berechnung von
$\mathtt{power}(m,n)$ auftreten kann. Der günstigste Fall tritt dann ein, wenn 
der Test in Zeile 6 immer gelingt weil $n$ jedesmal eine gerade Zahl ist.  In diesem Fall muss
es ein $l\in \N$ geben, so dass $n$ die Form \\[0.1cm]
\hspace*{1.3cm} $n = 2^l$ \\[0.1cm]
hat. Wir nehmen also $n = 2^l$ an und berechnen die Zahl $a_n$ der Multiplikationen, die
dann beim
Aufruf von $\mathtt{power}(m,n)$ durchgeführt werden. \\[0.1cm]
Zunächst gilt $a_{2^0} = a_1 = 2$, denn wenn $n = 1$  ist, scheitert der Test in Zeile 6 und Zeile 9
liefert 2 Multiplikationen.  Zeile 5 liefert in diesem Fall keine Multiplikation, weil
beim Aufruf $\mathtt{power}(m,0)$ sofort das Ergebnis in Zeile 4 zurück gegeben wird.

Ist $n = 2^l > 1$, so  haben wir in Zeile 7 eine Multiplikation, die zu den Multiplikationen, die beim
rekursiven Aufruf in Zeile $5$ anfallen, hinzu addiert werden muss.  Damit erhalten wir
die folgende Rekurrenz-Gleichung: \\[0.1cm]
\hspace*{1.3cm} $a_n = a_{n/2} + 1$ \qquad für alle $n \in \left\{2^l \mid l \in \N\right\}$\quad mit $a_1 = 2$. \\[0.1cm]
Wir definieren $b_l := a_{2^l}$ und erhalten dann für die Folge $(b_l)_l$ die
Rekurrenz-Gleichung \\[0.1cm]
\hspace*{1.3cm} 
$b_l = a_{2^l} = a_{(2^l)/2} + 1 = a_{2^{l-1}} + 1 = b_{l-1} + 1$ \qquad für alle $l\in\N$, \\[0.1cm]
mit der Anfangs-Bedingungen $b_0 = a_{2^0} = a_1 = 2$.
Also lösen wir die Rekurrenz-Gleichung \\[0.1cm]
\hspace*{1.3cm} $b_{l+1} = b_l + 1$ \qquad für alle $l \in \N$ \quad mit $b_0 = 2$.\\[0.1cm]
Offenbar lautet die Lösung \\[0.1cm]
\hspace*{1.3cm} $b_l = 2 + l$ \qquad für alle $l\in\N$.
\\[0.1cm]
Setzen wir hier $b_l = a_{2^l}$, so erhalten wir: \\[0.1cm]
\hspace*{1.3cm} $a_{2^l} = 2 + l$. \\[0.1cm]
Formen wir die Gleichung $n = 2^l$ nach $l$ um, so erhalten wir $l =
\log_2(n)$. Setzen wir diesen Wert 
ein, so sehen wir \\[0.1cm]
\hspace*{1.3cm} $a_n = 2 + \log_2(n) \in \Oh\bigl(\log_2(n)\bigr)$.
\vspace*{0.3cm}

Da wir sowohl im besten als auch im schlechtesten Fall das selbe Ergebnis bekommen haben,
können wir schließen, dass für die Zahl $a_n$ der Multiplikationen allgemein gilt:\\[0.1cm]
\hspace*{1.3cm} $a_n \in \Oh\bigl(\log_2(n)\bigr)$.
\vspace*{0.3cm}

\noindent
\textbf{Bemerkung}:  Wenn wir nicht die Zahl der Multiplikationen sondern die Rechenzeit
ermitteln wollen, die der obige Algorithmus benötigt, so wird die Rechnung wesentlich
aufwendiger.  Der Grund ist, dass wir dann berücksichtigen müssen, dass die Rechenzeit bei
der Berechnung der Produkte in den Zeilen 7 und 9 von der Größe der Faktoren abhängig ist.
\vspace*{0.3cm}

\exercise
Schreiben Sie eine Prozedur $\mathtt{prod}$ zur Multiplikation zweier Zahlen.
Für zwei natürliche Zahlen $m$ und $n$ soll der Aufruf $\mathtt{prod}(m, n)$  das Produkt
$m\cdot n$ mit Hilfe von Additionen
berechnen.  Benutzen Sie bei der Implementierung das Paradigma ``Teile und Herrsche'' und
beweisen Sie die Korrektheit des Algorithmus mit Hilfe einer Wertverlaufs-Induktion.
Schätzen  Sie die Anzahl der Additionen, die beim Aufruf von $\mathtt{prod}(m,n)$
im schlechtesten Fall durchgeführt werden, mit Hilfe der $\Oh$-Notation ab. 
\pagebreak

\section{Der Hauptsatz der Laufzeit-Funktionen}
Im letzten Abschnitt haben wir zur Analyse der Rechenzeit der Funktion $\textsl{power}()$
zunächst eine Rekurrenz-Gleichung aufgestellt, diese gelöst und anschließend das Ergebnis
mit Hilfe der $\Oh$-Notation abgeschätzt.  Wenn wir nur an einer Abschätzung interessiert
sind, dann ist es in vielen Fällen nicht notwendig, die zu Grunde liegende
Rekurrenz-Gleichung exakt zu lösen, denn der \textsl{Hauptsatz der Laufzeit-Funktionen}
(Englisch:~\textsl{Master Theorem}) \cite{cormen:01} bietet eine Methode zur Gewinnung von Abschätzungen,
bei der es nicht notwendig ist, die Rekurrenz-Gleichung zu lösen.  
Wir präsentieren eine etwas vereinfachte Form dieses Hauptsatzes.

\begin{Theorem}[Hauptsatz der Laufzeit-Funktionen] 
  Es seien 
  \begin{enumerate}
  \item $\alpha,\beta \in \mathbb{N}$ mit $\alpha \geq 1$ und $\beta > 1$,
  \item $f:\N \rightarrow \R_+$,
  \item die Funktion $g:\N \rightarrow \R_+$ genüge der Rekurrenz-Gleichung 
        \\[0.2cm]
        \hspace*{1.3cm}
        $g(n) = \alpha \cdot g\left(n/\beta\right) + f(n)$,
        \\[0.2cm]
        wobei der Ausdruck $n/\beta$ die ganzzahlige Division von $n$ durch $\beta$ bezeichnet.
  \end{enumerate}
  Dann können wir in den gleich genauer beschriebenen Situationen asymptotische
  Abschätzungen für die Funktion $g(n)$ angeben:
  \begin{enumerate}
  \item Falls es eine Konstante $\varepsilon > 0$ gibt, so dass 
        \\[0.2cm]
        \hspace*{1.3cm}
        $f(n) \in \Oh\bigl(n^{\log_\beta(\alpha) - \varepsilon}\bigr)$
        \\[0.2cm]
        gilt, dann haben wir 
        \\[0.2cm]
        \hspace*{1.3cm}
        $g(n) \in \Oh\left(n^{\log_\beta(\alpha)}\right)$.
  \item Falls sowohl $f(n) \in \Oh\bigl(n^{\log_\beta(\alpha)}\bigr)$ als auch $n^{\log_\beta(\alpha)} \in \Oh\bigl(f(n)\bigr)$
        gilt, dann folgt
        \\[0.2cm]
        \hspace*{1.3cm}
        $g(n) \in \Oh\bigl(\log_\beta(n) \cdot n^{\log_\beta(\alpha)}\bigr)$. 
  \item Falls es eine Konstante $\gamma < 1$ und eine Konstante $k \in \mathbb{N}$ gibt, so dass
        für $n \geq k$
        \\[0.2cm]
        \hspace*{1.3cm}
        $\alpha \cdot f\left(n/\beta\right) \leq \gamma \cdot f(n)$        
        \\[0.2cm]
        gilt, dann folgt 
        \\[0.2cm]
        \hspace*{1.3cm}
        $g(n) \in \Oh\bigl(f(n)\bigr)$. \hspace*{\fill} $\Box$
  \end{enumerate}
\end{Theorem}
\textbf{Erläuterung}:
Ein vollständiger Beweis dieses Theorems geht über den Rahmen einer einführenden Vorlesung hinaus.
Wir wollen aber erklären, wie die drei Fälle zustande kommen.
\begin{enumerate}
\item Wir betrachten zunächst den ersten Fall.  In diesem Fall kommt der asymptotisch 
wesentliche Anteil des Wachstums der Funktion $g$ von der Rekursion. 
Um diese Behauptung einzusehen, betrachten wir die homogene Rekurrenz-Gleichung
\[ g(n) = \alpha \cdot g\left(n/\beta\right). \]
Wir beschränken uns auf solche Werte von $n$, die sich als Potenzen von $\beta$ schreiben
lassen, also Werte der Form 
\[ n = \beta^k \quad \mbox{mit $k\in\N$.} \]
Definieren wir für $k \in \N$ die Folge $\bigl(b_k\bigr)_{k\in\N}$ durch
\[ b_k := g\bigl(\beta^k\bigr), \]
so erhalten wir für die Folgenglieder $b_k$ die Rekurrenz-Gleichung 
\[
     b_k = g\bigl(\beta^k\bigr) = \alpha \cdot g\left( \beta^k/\beta \right) 
   = \alpha \cdot g\bigl(\beta^{k-1}\bigr) = \alpha \cdot b_{k-1}.
\]
Wir sehen unmittelbar, dass diese Rekurrenz-Gleichung die Lösung 
\begin{equation}
  \label{eq:master}
  b_k = \alpha^k \cdot b_0   
\end{equation}
hat.  Aus $n = \beta^k$ folgt sofort 
\[  k = \log_\beta(n). \]
Berücksichtigen wir, dass $b_k = g(n)$ ist, so liefert Gleichung (\ref{eq:master}) also 
\begin{equation}
  \label{eq:master2}
   g(n) = \alpha^{\log_\beta(n)} \cdot b_0.   
\end{equation}
Wir zeigen, dass
\begin{equation}
  \label{eq:master1}
  \alpha^{\log_\beta(n)} = n^{\log_\beta(\alpha)} 
\end{equation}
gilt.  Dazu betrachten wir die folgende Kette von Äquivalenz-Umformungen:
\[ 
\begin{array}[t]{llcll}
                & \alpha^{\log_\beta(n)} & = & n^{\log_\beta(\alpha)} & \mid\; \log_\beta(\cdot)  \\[0.2cm]
\Leftrightarrow & \log_\beta\left(\alpha^{\log_\beta(n)}\right) 
                  & = & \log_\beta\left(n^{\log_\beta(\alpha)}\right) \\[0.2cm]
\Leftrightarrow & \log_\beta(n) \cdot log_\beta(\alpha) 
                  & = & \log_\beta(\alpha) \cdot \log_\beta(n) & 
                  \mbox{wegen $\log_b(x^y) = y \cdot \log_b(x)$}
\end{array}
\]
Da die letzte Gleichung offenbar richtig ist, und wir zwischendurch nur Äquivalenz-Umformungen
durchgeführt haben, ist auch die erste Gleichung richtig und wir haben Gleichung (\ref{eq:master1}) gezeigt.
Insgesamt haben wir damit 
\[ g(n) = n^{\log_\beta(\alpha)} \cdot b_0 \]
gezeigt.  Also gilt: Vernachlässigen wir die Inhomogenität $f$, so erhalten wir
die folgende asymptotische Abschätzung:
\[ g(n) \in \Oh\bigl(n^{\log_\beta(\alpha)}\bigr). \]
\item Im zweiten Fall liefert
die Inhomogenität $f$ einen Beitrag, der genau so groß ist wie die Lösung der homogenen
Rekurrenz-Gleichung.  Dies führt dazu, dass die Lösung asymptotisch um einen Faktor
$\log_\beta(n)$ größer wird.  Um das zu verstehen, betrachten wir exemplarisch die Rekurrenz-Gleichung
\[ g(n) = \alpha \cdot g\left(n/\beta\right) + n^{\log_\beta(\alpha)} \]
mit der Anfangs-Bedingung $g(1) = 0$.  Wir betrachten wieder nur Werte 
$n \in \{ \beta^k \mid k \in \N \}$ und setzen daher
\[ n = \beta^k. \]
Wie eben definieren wir
\[ b_k := g(n) = g\bigl(\beta^k\bigr). \]
Das liefert
\[ b_k = \alpha \cdot g\left(\beta^k/\beta\right) + \bigl(\beta^k\bigr)^{\log_\beta(\alpha)}
       = \alpha \cdot g(\beta^{k-1}) + \left(\beta^{\log_\beta(\alpha)}\right)^k
       = \alpha \cdot b_{k-1} + \alpha^k.
 \]
Nun gilt $b_0 = g(1) = 0$.  Um die Rekurrenz-Gleichung $b_k = \alpha \cdot b_{k-1} + \alpha^k$ zu lösen,
berechnen wir zunächst die Werte für $k=1,2,3$:
\begin{eqnarray*}
  b_1 & = & \alpha \cdot b_0 + \alpha^1               \\
      & = & \alpha \cdot 0 + \alpha                   \\
      & = & 1 \cdot \alpha^1                          \\[0.2cm]
  b_2 & = & \alpha \cdot b_1 + \alpha^1               \\
      & = & \alpha \cdot 1 \cdot \alpha^1 + \alpha^2  \\
      & = & 2 \cdot \alpha^2                          \\[0.2cm]
  b_3 & = & \alpha \cdot b_2 + \alpha^2               \\
      & = & \alpha \cdot 2 \cdot \alpha^2 + \alpha^3  \\
      & = & 3 \cdot \alpha^3                          
\end{eqnarray*}
Wir vermuten hier, dass die Lösung dieser Rekurrenz-Gleichung durch die Formel
\[ b_k = k \cdot \alpha^k \]
gegeben wird.  Den Nachweis dieser Vermutung führen wir durch eine triviale Induktion:
\begin{enumerate}
\item[I.A.:] $k = 0$
             
             Einerseits gilt $b_0 = 0$, andererseits gilt $0 \cdot \alpha^0 = 0$.

\item[I.S.:] $k \mapsto k+1$

            \[
            \begin{array}[t]{lcl}
              b_{k+1} &               =  & \alpha \cdot b_k + \alpha^{k+1}              \\[0.1cm] 
                      & \stackrel{IV}{=} & \alpha \cdot k \cdot \alpha^k + \alpha^{k+1} \\[0.1cm]  
                      &               =  & k \cdot \alpha^{k+1} + \alpha^{k+1}          \\[0.1cm] 
                      &               =  & (k + 1) \cdot \alpha^{k+1}. 
            \end{array}
            \]
\end{enumerate}
Da aus $n = \beta^k$ sofort $k = \log_\beta(n)$ folgt, ergibt sich für die Funktion
$g(n)$ 
\[ g(n) = b_k = k \cdot \alpha^k = \log_\beta(n) \cdot \alpha^{\log_\beta(n)} = \log_\beta(n) \cdot
n^{\log_\beta(\alpha)} \]
und das ist genau die Form, durch die im zweiten Fall des Hauptsatzes die Funktion
$g(n)$ abgeschätzt wird.
\item
Im letzten Fall des Hauptsatzes überwiegt schließlich der Beitrag der Inhomogenität, so dass die Lösung 
nun asymptotisch durch die Inhomogenität dominiert wird.
Wir machen wieder den Ansatz 
\[ n = \beta^k \quad \mbox{und} \quad b_k = g\bigl(\beta^k\bigr). \]
Wir überlegen uns, wie die Ungleichung 
\[ \alpha \cdot f\left(n/\beta\right) \leq \gamma \cdot f(n) \]
für $n = \beta^k$ aussieht und erhalten
\begin{equation}
  \label{eq:master_u1}
 \alpha \cdot f\left(\beta^{k-1}\right) \leq \gamma \cdot f\bigl(\beta^k\bigr) 
\end{equation}
Setzen wir hier für $k$ den Wert $k-1$ ein, so erhalten wir
\begin{equation}
  \label{eq:master_u2}
 \alpha \cdot f\left(\beta^{k-2}\right) \leq \gamma \cdot f\bigl(\beta^{k-1}\bigr) 
\end{equation}
Wir multiplizieren nun die Ungleichung (\ref{eq:master_u2}) mit $\alpha$ und
Ungleichung (\ref{eq:master_u1}) mit $\gamma$ und erhalten die Ungleichungen
\[ \alpha^2 \cdot f\left(\beta^{k-2}\right) \leq \alpha \cdot \gamma \cdot f\bigl(\beta^{k-1}\bigr) 
   \quad \mbox{und} \quad
   \alpha \cdot \gamma \cdot f\left(\beta^{k-1}\right) \leq \gamma^2 \cdot f\bigl(\beta^k\bigr) 
\]
Setzen wir diese Ungleichungen zusammen, so erhalten wir die neue Ungleichung
\[ \alpha^2 \cdot f\left(\beta^{k-2}\right) \leq \gamma^2 \cdot f\bigl(\beta^k\bigr) \]
Iterieren wir diesen Prozess, so sehen wir, dass 

\begin{equation}
  \label{eq:master_u3}
\alpha^i \cdot f\bigl(\beta^{k-i}\bigr) \leq \gamma^i \cdot f(\beta^k) 
   \quad \mbox{für alle $i \in \{1,\cdots k\}$ gilt.}   
\end{equation}
Wir berechnen nun $g(\beta^k)$ durch Iteration der Rekurrenz-Gleichung:
\[
\begin{array}[t]{lcl}
g(\beta^k) & = & \alpha \cdot g(\beta^{k-1}) + f(\beta^k) \\
           & = & \alpha \cdot \bigl(\alpha \cdot g(\beta^{k-2}) + f(\beta^{k-1})\bigr) + f(\beta^k) \\
           & = & \alpha^2 \cdot g(\beta^{k-2}) + \alpha \cdot f(\beta^{k-1}) + f(\beta^k) \\
           & = & \alpha^3 \cdot g(\beta^{k-3}) + 
                 \alpha^2 \cdot f(\beta^{k-2}) + \alpha \cdot f(\beta^{k-1}) + f(\beta^k) \\
           &   & \vdots \\
           & = & \alpha^k \cdot g(\beta^0) + \alpha^{k-1} \cdot f(\beta^1) + \cdots +
                 \alpha^1 \cdot f(\beta^{k-1}) + \alpha^0 \cdot f(\beta^k) \\
           & = & \alpha^k \cdot g(\beta^0) + \sum\limits_{i=1}^{k} \alpha^{k-i} \cdot f(\beta^i)
\end{array}
\]
Da bei der $\Oh$-Notation die Werte von $f$ für kleine Argumente keine Rolle spielen,
können wir ohne Beschränkung der Allgemeinheit annehmen, dass $g(\beta^0) \leq f(\beta^0)$
ist.  Damit erhalten wir dann die Abschätzung
\[ 
\begin{array}{lcl}
g(\beta^k) & \leq & \alpha^k \cdot f(\beta^0) + \sum\limits_{i=1}^{k} \alpha^{k-i} \cdot f(\beta^i) \\[0.3cm]
           & =    & \sum\limits_{i=0}^{k} \alpha^{k-i} \cdot f(\beta^i) \\[0.3cm]
           & =    & \sum\limits_{j=0}^{k} \alpha^{j} \cdot f(\beta^{k-j}) 
\end{array}
\]
wobei wir im letzten Schritt den Index $i$ durch $k-j$ ersetzt haben.  Berücksichtigen wir
nun die Ungleichung (\ref{eq:master_u3}), so erhalten wir die Ungleichungen
\[
\begin{array}{lcl}
   g(\beta^k) & \leq & \sum\limits_{j=0}^{k} \gamma^j \cdot f(\beta^k) \\[0.3cm]
              & =    & f(\beta^k) \cdot \sum\limits_{j=0}^{k} \gamma^j \\[0.5cm]
              & \leq & f(\beta^k) \cdot \sum\limits_{j=0}^{\infty} \gamma^j \\[0.5cm]
              & =    & f(\beta^k) \cdot \bruch{1}{\;1 - \gamma\;},
\end{array}
\]
wobei wir im letzten Schritt die Formel für die geometrische Reihe 
\[ \sum\limits_{j=0}^{\infty} q^j = \bruch{1}{1 - q} \]
benutzt haben.  Ersetzen wir nun $\beta^k$ wieder durch $n$, so sehen wir, dass 
\[ g(n) \leq \bruch{1}{\;1 - \gamma\;} \cdot f(n) \]
gilt und daraus folgt sofort
\[ g(n) \in \Oh\bigl(f(n)\bigr). \hspace*{10cm} \Box \]
\end{enumerate}
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}:
Wir untersuchen das asymptotische Wachstum der Folge, die durch die Rekurrenz-Gleichung
\[ a_n = 9 \cdot a_{n/3} + n \]
definiert ist.  Wir haben hier 
\[ g(n) = 9 \cdot g(n/3) + n, \quad \mbox{also} \quad \alpha = 9, 
   \quad \beta = 3, \quad f(n) = n.
\]
Damit gilt 
\[ \log_\beta(\alpha) = \log_3(9) = 2. \]
Wir setzen $\varepsilon := 1 > 0$.  Dann gilt
\[ f(n) = n \in \Oh(n) = \Oh\left(n^{2-1}\right) = \Oh\left(n^{2-\varepsilon}\right). \]
Damit liegt der erste Fall des Hauptsatzes vor und wir können schließen, dass
\[ g(n) \in \Oh(n^2) \]
gilt. \qed
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}:
Wir  betrachten die Rekurrenz-Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$a_n = a_{n/2} + 2$
\\[0.2cm]
und analysieren das asymptotische Wachstum der Funktion $n \mapsto a_n$ mit Hilfe des
Hauptsatzes der Laufzeit-Funktionen.
Wir setzen $g(n) := a_n$ und haben also für die Funktion $g$ die Rekurrenz-Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$g(n) = 1 \cdot g\left(n/2\right) + 2$
\\[0.2cm]
Wir definieren $\alpha := 1$, $\beta := 2$ und $f(n) = 2$.  Wegen 
\\[0.2cm]
\hspace*{1.3cm}
$\log_\beta(\alpha) = \log_2(1) = 0$ \quad und \quad
$2 \in \Oh(1)= \Oh(n^0)$ \quad sowie \quad $n^0 \in \Oh(2)$
\\[0.2cm]
sind die Voraussetzungen des zweiten Falls erfüllt und wir erhalten 
\\[0.2cm]
\hspace*{1.3cm}
$a_n \in \Oh\bigl(\log_2(n)\bigr)$.
\qed
\vspace*{0.3cm}

\noindent
\textbf{Beispiel}:
Diesmal betrachten  wir die Rekurrenz-Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$a_n = 3 \cdot a_{n/4} + n \cdot \log_2(n)$.
\\[0.2cm]
Es gilt $\alpha = 3$, $\beta = 4$ und $f(n) = n \cdot \log_2(n)$.  Damit gilt
\[ log_\beta(\alpha) = \log_4(3) < 1. \]
Damit ist klar, dass die Funktion $f(n) = n \cdot log_2(n)$ schneller wächst als die
Funktion $n^{\log_4(3)}$.  Damit kann höchstens der dritte Fall des Hauptsatzes vorliegen.
Wir suchen also ein $\gamma < 1$, so dass die Ungleichung
\[ \alpha \cdot f(n/\beta) \leq \gamma \cdot f(n) \]
gilt.  Setzen wir hier die Funktion $f(n) = n \cdot \log_2(n)$ und die Werte für $\alpha$
und $\beta$ ein, so erhalten wir die Ungleichung 
\[ 3 \cdot n/4 \cdot \log_2(n/4) \leq \gamma \cdot n \cdot \log_2(n), \]
die für durch 4 teilbares $n$ offenbar äquivalent ist zu
\[ \frac{3}{4} \cdot \log_2(n/4) \leq \gamma \cdot \log_2(n). \]
Setzen wir $\gamma := \frac{3}{4}$ und kürzen, so geht diese Ungleichung über in die
offensichtlich wahre Ungleichung
\[ \log_2(n/4) \leq \log_2(n). \] 
Damit liegt also der dritte Fall des Hauptsatzes vor und wir können schließen, dass
\[ a_n \in \Oh\left(n \cdot \log_2(n)\right) \]
gilt. \qed
\vspace*{0.3cm}

\exercise
Benutzen Sie den Hauptsatz der Laufzeit-Funktionen um das asymptotische Wachstum 
der Folgen $\folge{a_n}$, $\folge{b_n}$ und $\folge{c_n}$  abzuschätzen, falls diese
Folgen den nachstehenden Rekurrenz-Gleichungen genügen:
\begin{enumerate}
\item $a_n = 4 \cdot a_{n/2} + 2 \cdot n + 3$.
\item $b_n = 4 \cdot b_{n/2} + n^2$.
\item $c_n = 3 \cdot c_{n/2} + n^3$.
\end{enumerate}

\noindent
\textbf{Bemerkung}: Es ist wichtig zu sehen, dass die drei Fälle des Theorems nicht vollständig sind:
Es gibt Situationen, in denen der Hauptsatz nicht anwendbar ist.  Beispielsweise lässt sich der Hauptsatz nicht
für die Funktion $g$, die durch die Rekurrenz-Gleichung
\[ g(n) = 2 \cdot g(n/2) + n \cdot \log_2(n) \quad \mbox{mit der Anfangs-Bedingung $g(1) = 0$}\]
definiert ist, anwenden, denn die Inhomogenität wächst schneller als im zweiten Fall, aber
nicht so schnell, dass der dritte Fall vorliegen würde.  Dies können wir wie folgt sehen.
Es gilt 
\[ \alpha = 2, \quad \beta = 2 \quad \mbox{und damit} \quad \log_\beta(\alpha) = 1. \]
Damit der zweite Fall vorliegt, müsste 
\[ n \cdot \log_2(n) \in \Oh(n^1) \]
gelten, was sicher falsch ist.  Da die Inhomogenität $n \cdot \log_2(n)$ offenbar
schneller wächst als der Term $n^1$, kann jetzt höchstens noch der dritte Fall vorliegen.
Um diese Vermutung zu überprüfen, nehmen wir an, dass ein $\gamma < 1$ existiert, so dass die Inhomogenität 
\[ f(n) := n \cdot \log_2(n) \]
die Ungleichung 
\[ \alpha \cdot f(n/\beta) \leq \gamma \cdot f(n) \]
erfüllt.  Einsetzen von $f$ sowie von $\alpha$ und $\beta$ führt auf die Ungleichung
\[ 2 \cdot n/2 \cdot \log_2(n/2) \leq \gamma \cdot n \cdot \log_2(n). \]
Dividieren wir diese Ungleichung durch $n$ und vereinfachen, so erhalten wir
\[ \log_2(n) - \log_2(2) \leq \gamma \cdot \log_2(n). \]
Wegen $\log_2(2) = 1$ addieren wir auf beiden Seiten 1 und subtrahieren $\gamma \cdot \log_2(n)$.
Dann erhalten wir
\[ \log_2(n) \cdot (1 - \gamma) \leq 1, \]
woraus schließlich 
\[ \log_2(n) \leq \bruch{1}{\;1 - \gamma\;} \]
folgt.  Daraus folgt durch Anwenden der Funktion $x \mapsto 2^x$ die Ungleichung
\[ \displaystyle n \leq 2^\frac{1}{\;1 - \gamma\;}, \]
die aber sicher nicht für beliebige $n$ gelten kann.  Damit haben wir einen Widerspruch
zu der Annahme, dass der dritte Fall des Hauptsatzes vorliegt.
\hspace*{\fill} $\Box$
\vspace*{0.3cm}

\exercise
Lösen Sie die Rekurrenz-Gleichung
\[ g(n) = 2 \cdot g(n/2) + n \cdot \log_2(n) \quad \mbox{mit der Anfangs-Bedingung $g(1) = 0$} \]
für den Fall, dass $n$ eine Zweier-Potenz ist.
\hspace*{\fill} $\Box$


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "algorithmen"
%%% End: 
