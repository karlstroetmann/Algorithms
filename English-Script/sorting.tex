\chapter{Sorting}
In this chapter, we assume that we have been given a list $l$.  The elements of $l$ are members of
some set $s$.  If we want to \emph{sort} the list $l$ we have to be able to compare these elements
to each other.   Therefore, we assume that $s$ is equipped with a binary relation $\leq$ which is
\emph{reflexive}, \emph{anti-symmetric} and \emph{transitive}, i.~e.~we have  
\begin{enumerate}
\item $\forall x \el s \colon x \leq x$,
\item $\forall x, y \el s \colon \bigl(x \leq y \wedge y \leq x
  \rightarrow x = y\bigr)$, 
\item $\forall x, y, z \el s \colon \bigl(x \leq y \wedge y \leq z \rightarrow x \leq z\bigr)$. 
\end{enumerate}
A pair $\langle s, \leq \rangle$ where $s$ is a set and $\leq \;\subseteq s \times s$ is a relation
on $s$ that is \emph{reflexive}, \emph{anti-symmetric} and \emph{transitive} is called a
\href{http://en.wikipedia.org/wiki/Partially_ordered_set}{\emph{partially ordered set}}.  
If, furthermore
\\[0.2cm]
\hspace*{1.3cm}
$\forall x, y \el s \colon\bigl( x \leq y \vee y \leq x\bigr)$
\\[0.2cm]
holds, then the pair $\langle s, \leq \rangle$ is called a 
\href{http://en.wikipedia.org/wiki/Totally_ordered_set}{\emph{totally ordered set}}.

\examples
\begin{enumerate}
\item $\langle\mathbb{N}, \leq \rangle$ is a totally ordered set.
\item $\langle 2^{\mathbb{N}}, \subseteq \rangle$ is a partially ordered set but it is not a totally
      ordered set.  For example, the sets $\{1\}$ and $\{2\}$ are not comparable since we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\{1\} \not\subseteq \{2\}$ \quad and \quad  $\{2\} \not\subseteq \{1\}$.
\item If $P$ is the set of employees of some company and if we define for given employees
      $a,b \el P$
      \\[0.2cm]
      \hspace*{1.3cm}
      $a \preceq b$ \quad iff \quad  $a$ does not earn more than $b$, 
      \\[0.2cm]
      then the $\langle P, \leq \rangle$ is not a partially ordered set.  The reason is that
      the relation $\preceq$ is not anti-symmetric:  If Mr.~Smith earns as much as
      Mrs.~Robinson, then we have both
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{Smith} \preceq \mathrm{Robinson}$ \quad and \quad $\mathrm{Robinson} \preceq \mathrm{Smith}$
      \\[0.2cm]
      but obviously $\mathrm{Smith} \not= \mathrm{Robinson}$.
\end{enumerate}
In the examples given above we see that it does not make sense to sort subsets of $\mathbb{N}$.
However, we can sort natural numbers with respect to their size and we can also sort employees with
respect to their income.  This show that, in order to sort,  we do not necessarily need a totally
ordered set.  In order to capture the requirements that are needed to be able to sort we introduce
the notion of a \href{http://en.wikipedia.org/wiki/Preorder}{\emph{quasiorder}}.


\begin{Definition}[Quasiorder]  \hspace*{\fill} \\
{\em
  A pair $\langle s, \preceq\rangle$ is a \emph{quasiorder}  if $\preceq$ is a 
  binary relation on $m$ such that we have the following:
  \begin{enumerate}
  \item $\forall x \el s\colon x \preceq x$. \hspace*{\fill} (reflexivity)
  \item $\forall x, y, z \el s \colon \bigl(x \preceq y \wedge y \preceq z \rightarrow x \preceq z\bigr)$. 
         \hspace*{\fill} (transitivity)
  \end{enumerate}
  If, furthermore,
  \\[0.2cm]
  \hspace*{1.3cm}
  $\forall x, y \el s \colon \bigl(x \preceq y \vee y \preceq x\bigr)$
  \\[0.2cm]
  holds, then $\langle s, \preceq \rangle$ is called a \emph{total quasiorder}.  This will be
  abbreviated as \textsc{TQO}.
}
\end{Definition}
A quasiorder $\langle s, \preceq \rangle$ does not require the relation $\preceq$ to be
anti-symmetric.  Nevertheless, the notion of a quasiorder is very closely related to the notion of a
partial order.  The reason is as follows:  If $\langle s, \preceq \rangle$ is a quasiorder, then we
can define an equivalence relation $\approx$ on $s$ by setting
\\[0.2cm]
\hspace*{1.3cm}
$x \approx y \stackrel{\mbox{\scriptsize def}}{\Longleftrightarrow} x \preceq y \wedge y \preceq x$. 
\\[0.2cm]
If we extend the order $\preceq$ to the equivalence classes generated by the relation $\approx$,
then it can be shown that this extension is a partial order.
\vspace*{0.3cm}

Let us assume that $\langle m, \preceq \rangle$ is a  \textsc{TQO}.  Then the \emph{sorting problem}
is defined as follows:
\begin{enumerate}
\item A list $l$ of elements of $m$ is given.
\item We want to compute a list $s$ such that we have the following: 
  \begin{enumerate}
  \item $s$ is sorted ascendingly: \\[0.2cm]
        \hspace*{1.3cm} 
        $\forall i \el \{ 1, \cdots, \#s-1 \} \colon s[i] \preceq s[i+1]$ 
        \\[0.2cm]
        Here, the length of the list $s$ is denoted as $\#s$ and $s[i]$ is the $i$-th element of $s$.
  \item The elements of $m$ occur in $l$ and $s$ with the same frequency: \\[0.2cm]
        \hspace*{1.3cm} 
        $\forall x\el m \colon \textsl{count}(x,l) = \textsl{count}(x,s)$.
        \\[0.2cm]
        Here, the function $\textsl{count}(x,l)$ returns the number of occurrences of $x$ in $l$.
        Therefore, we have: \\[0.2cm]
        \hspace*{1.3cm}
        $\textsl{count}(x,l) := \# \bigl\{ i \el \{1,\cdots,\#l\} \mid l[i] = x \bigr\}$.
  \end{enumerate}
\end{enumerate}
Next, we present various algorithms for solving the sorting problem.  We start with two algorithms
that are very easy to implement: \emph{insertion sort} and \emph{selection sort}.  However, the
efficiency of these algorithms is far from optimal.  Next, we present \emph{quick sort} and 
\emph{merge sort}.  Both of these algorithms are very efficient when implemented carefully.
However, the implementation of these algorithms is much more involved.


\section{Insertion Sort}
Let us start our investigation of sorting algorithms with
\href{http://en.wikipedia.org/wiki/Insertion_sort}{\emph{insertion sort}}.  We will describe the
algorithm via a set of equations.
\begin{enumerate}
\item If the list $l$ that has to be sorted is empty, then the result is the empty list: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{sort}([]) = []$.
\item Otherwise, the list $l$ must have the form $[x] + r$. Here, $x$ is the first element of $l$
      and $r$ is the rest of $l$, i.~e.~everything of $l$ but the first element.  In order to sort
      $l$ we first sort the rest $r$ and then we insert the element $x$ into the resulting list in a
      way that the resulting list remains sorted:
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}\bigl([x] + r\bigr) = \mathtt{insert}\bigl(x, \mathtt{sort}(r)\bigr)$.
\end{enumerate}
Inserting $x$ into an already sorted list $s$ is done according to the following specification:
\begin{enumerate}
\item If $s$ is empty, the result is the list $[x]$: \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{insert}(x,[]) = [x]$.
\item Otherwise, $s$ must have the form $[y] + r$.  In order to know where to insert $x$ we have to
      compare $x$ and $y$.
      \begin{enumerate}
      \item If $x \preceq y$, then we have to insert $x$ at the front of the list $s$: \\[0.2cm]
            \hspace*{1.3cm}
            $x \preceq y \rightarrow \mathtt{insert}\bigl(x, [y] + r\bigr) = [x,y] + r$. 
      \item Otherwise, $x$ has to be inserted recursively into the list $r$: \\[0.2cm]
            \hspace*{1.3cm}
            $\neg x \preceq y \rightarrow \mathtt{insert}\bigl(x, [y] + r\bigr) = [y] + \mathtt{insert}(x,r)$. 
      \end{enumerate}
\end{enumerate}

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.3cm,
                  xrightmargin  = 0.3cm
                ]
    sort := procedure(l) {
        match (l) {
            case []   : return [];
            case [x|r]: return insert(x, sort(r));
        }
    };
    insert := procedure(x, l) {
        match (l) {
            case []                : return [x];
            case [y|r] |   x <= y  : return [x,y] + r;
            case [y|r] | !(x <= y) : return [y] + insert(x, r);
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementing \emph{insertion sort} in \textsc{SetlX}.}
  \label{fig:insertion-sort.stlx}
\end{figure} 

\noindent
Figure \ref{fig:insertion-sort.stlx} shows how the \emph{insertion-sort} algorithm can be implemented 
in \textsc{SetlX}.
\begin{enumerate}
\item The definition of the function \texttt{sort} makes use of the \texttt{match} statement
      that is available in \texttt{SetlX}.  Essentially, the \texttt{match} statement is an upgraded
      \texttt{switch} statement.  Therefore, line 3 is executed if the list $l$ is empty.

      Line 4 tests whether $l$ can be written as  
      \\[0.2cm]
      \hspace*{1.3cm}
      $l = [ x ] + r$.
      \\[0.2cm]
      Here, $x$ is the first element of $l$ while $r$ contains all but the first element of $l$.  
      In \textsc{SetlX} we write \texttt{[x|r]} in order to match a list of the form $[x] + r$.
\item The definition of the function $\mathtt{insert}$ also uses a 
      \texttt{match} statement.  However, in the last two cases the match statement
      also has a logical condition attached via the operator ``\texttt{|}'':
      In line 10, this condition checks whether $x \leq y$, while line 11 checks for the
      complementary case.
\end{enumerate}

\subsection{Complexity of Insertion Sort}
We will compute the number of comparisons that are done in the implementation of \texttt{insert}.
Before doing so, let us note that the function \texttt{insert.stlx} can be rewritten as shown in Figure
\ref{fig:insert.stlx}.  In comparison to Figure \ref{fig:insertion-sort.stlx}, we have dropped the
test ``\texttt{!(x <= y)}'' from line 5 since it is unnecessary:  If control reaches line 5, it must
have skipped line 4 before and for a non-empty list that can only happen if the test 
``\texttt{x <= y}'' fails.
\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm
                ]
    insert := procedure(x, l) {
        match (l) {
            case []             : return [x];
            case [y|r] | x <= y : return [x, y] + r;
            case [y|r]          : return [y] + insert(x, r);
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{More efficient implementation of \emph{insert}.}
  \label{fig:insert.stlx}
\end{figure} 

Let us compute the number of evaluations of the comparison operator ``\texttt{<=}'' in line 4 in the
worst case if we call $\texttt{sort}(l)$ with a list of length $n$. In order to do that, we have to
compute the number of evaluations of the operator ``\texttt{<=}'' when 
 $\texttt{insert}(x,l)$ is evaluated for a list $l$ of length $n$.  Let us denote this number as 
$a_n$.  The worst case happens if $x$ is bigger than every element of $l$ because in that case the
test ``\texttt{x <= y}'' in line 4 of Figure \ref{fig:insert.stlx} will always evaluate to
\texttt{false} and therefore \texttt{insert} will keep calling itself recursively.
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$a_0 = 0$ \quad and \quad $a_{n+1} = a_n + 1$. 
\\[0.2cm]
A trivial induction shows that this recurrence relation has the solution
\\[0.2cm]
\hspace*{1.3cm} 
$a_n = n$.
\\[0.2cm]
In the worst case the evaluation of $\mathtt{insert}(x,l)$ will lead to $n$ comparisons for a list
$l$ of length $n$.  The reason is simple:  If $x$ is bigger than any element of $l$, then we have to
compare $x$ with every element of $l$ in order to insert it into $l$.

Next, let us compute the number of comparisons that have to be done when calling
$\texttt{sort}(l)$ in the worst case for a list  $l$ of length $n$.  Let us denote this number as
$b_n$. The worst case happens if $l$ is sorted in reverse order, i.~e.~if $l$ is sorted
descendingly.   
Then we have \\[0.2cm]
\hspace*{1.3cm}
 $b_1 = 0$ \quad and \quad $b_{n+1} = b_n + n$, \hspace*{\fill} (1)
\\[0.2cm]
because for a list of the form $l = [x] + r$ of length $n+1$ we first have to sort the list $r$
recursively.  As $r$ has length $n$ this takes $b_n$ comparisons.  After that, the call
$\mathtt{insert}(x, \mathtt{sort(r)})$ 
inserts the element $x$ into $\mathtt{sort}(r)$.  We have previously seen that this takes $n$
comparisons if $x$ is bigger than all elements of $\mathtt{sort}(l)$ and if the list $l$ is sorted
descendingly this will indeed be the case.

If we substitute $n$ by $n-1$ in equation $(1)$ we find
\\[0.2cm]
\hspace*{1.3cm}
$b_n = b_{n-1} + (n - 1)$.
\\[0.2cm]
This recurrence equation is solved by expanding the right hand side successively as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  b_{n} & = & b_{n-1} + (n - 1)                     \\ 
        & = & b_{n-2} + (n - 2) + (n - 1)           \\ 
        & \vdots &                                  \\
        & = & b_{n-k} + (n - k) + \cdots + (n - 1)  \\ 
        & \vdots &                                  \\
        & = & b_{1} + 1 + \cdots + (n - 1)      \\[0.2cm] 
        & = & b_{1} + \sum\limits_{i = 1}^{n - 1} i \\[0.4cm]
        & = & \frac{1}{2} \cdot n \cdot (n - 1),
\end{array}
$
\\[0.2cm]
because $b_1 = 0$ and the sum of all natural numbers from 1 upto  $n - 1$ is given as
\\[0.2cm]
\hspace*{1.3cm}
$\sum\limits_{i = 0}^{n - 1} i  = \frac{1}{2} \cdot n \cdot (n - 1)$.
\\[0.2cm]
This can be shown by a straightforward induction.  Therefore, in the worst case the number $b_n$ of
comparisons needed for sorting a list of length $n$  satisfies 
\\[0.2cm]
\hspace*{1.3cm}
$b_n = \frac{1}{2} \cdot n^2 - \frac{1}{2} \cdot n = \frac{1}{2} \cdot n^2 + \Oh(n)$.
\\[0.2cm]
Therefore, in the worst case the number of comparisons is given as $\Oh(n^2)$ and hence
\emph{insertion sort} is quadratic.


Next, let us consider the best case.  The best case happens if the list $l$ is already sorted
ascendingly.  Then, the call of 
$\mathtt{insert}(x,\mathtt{sort}(r))$ only needs a single comparison.  This time, the recurrence
equation for the number $b_l$ of comparisons when sorting $l$ satisfies
 \\[0.2cm]
\hspace*{1.3cm}
$b_1 = 0$ \quad and \quad $b_{n+1} = b_n + 1$. 
\\[0.2cm]
Obviously, the solution of this recurrence equation is $b_n = n-1$.  Therefore, in the best case
\emph{insertion sort} is linear.  This is as good as it can get because when sorting a list $l$ we
must at least inspect all of the elements of $l$ and therefore we will always have at least a linear
amount of work to do.


\section{Selection Sort}
Next, we discuss 
\href{http://en.wikipedia.org/wiki/Selection_sort}{\emph{selection sort}}.  In order to sort a given
list $l$ this algorithms works as
follows:
\begin{enumerate}
\item If $l$ is empty, the result is the empty list: \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}([]) = []$.
\item Otherwise, we compute the smallest element of the list $l$ and we remove this element from
      $l$.  Next, the remaining list is sorted recursively.  Finally, the smallest element is added
      to the front of the sorted list:
      \\[0.2cm]
      \hspace*{1.3cm} 
      $l \not= [] \rightarrow \mathtt{sort}\bigl(l\bigr) = \bigl[\texttt{min}(l)\bigr]
      + \mathtt{sort}\bigl(\mathtt{delete}(\texttt{min}(l), l)\bigr)$.
\end{enumerate}
The algorithm to delete an element $x$ from a list $l$ is formulated recursively.  There are three cases:
\begin{enumerate}
\item If $l$ is empty, we have \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{delete}(x, []) = []$.
\item If $x$ is equal to the first element of $l$, then the function \texttt{delete} returns the
      rest of $l$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{delete}(x, [x] + r) = r$.
\item Otherwise, the element $x$ is removed recursively from the rest of the list: \\[0.2cm]
      \hspace*{1.3cm}   
      $x \not = y \rightarrow \mathtt{delete}(x, [y] + r) = [y] + \mathtt{delete}(x,r)$.
\end{enumerate}
Finally, we have to specify the computation of the minimum of a list $l$:
\begin{enumerate}
\item The minimum of the empty list is bigger than any element.  Therefore we have 
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{min}([]) = \infty$.
\item In order to compute the minimum of the list $[x] + r$ we compute the minimum of $r$ and
      then use the binary function \texttt{min}: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{min}([x] + r) = \mathtt{min}\bigl(x, \mathtt{min}(r) \bigr)$. 

      Here, the binary function \texttt{min} is defined as follows: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{min}(x,y) = \left\{
      \begin{array}{ll}
        x  & \mbox{if $x \preceq y\,$;} \\
        y  & \mbox{otherwise.} \\
      \end{array}\right.
      $
\end{enumerate}
Figure \ref{fig:selection-sort.setlx} on page \pageref{fig:selection-sort.setlx} shows an
implementation of selection sort in \textsc{SetlX}.  There was no need to implement the function \texttt{min} as this function is already predefined in \textsc{SetlX}.
The implementation of $\mathtt{delete}(x,l)$ is \emph{defensive}:  Normally, $\mathtt{delete}(x, l)$
should only be called if $x$ is indeed an element of the list $l$.   Therefore, there must be
a mistake if we try to delete an element from the empty list.  The predefined \texttt{assert}
function will provide us with an error message in this case.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.3cm,
                  xrightmargin  = 0.3cm
                ]
    sort := procedure(l) {
        if (l == []) {
            return [];
        }
        x := min(l);
        return [x] + sort(delete(x,l));
    };   
    delete := procedure(x, l) {
        match (l) {
            case []    : assert(false, "$x$ not in list $l$");
            case [x|r] : return r;
            case [y|r] : return [y] + delete(x,r);
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementing \emph{selection sort} in \textsc{SetlX}.}
  \label{fig:selection-sort.setlx}
\end{figure}

\subsection{Complexity of Selection Sort}
In order to be able to analyze the complexity of \emph{selection sort} we have to count the number
of comparisons that are performed when $\mathtt{min}(l)$ is computed.  We have 
\\[0.2cm]
\hspace*{1.3cm} 
$\mathtt{min}([x_1,x_2,x_3,\cdots,x_n]) = \mathtt{min}(x_1, \mathtt{min}(x_2, \mathtt{min}(x_3, \cdots \mathtt{min}(x_{n-1},x_n) \cdots )))$. 
\\[0.2cm]
Therefore, in order to compute $\texttt{min}(l)$ for a list $l$ of length $n$ the binary function \texttt{min}
is called $(n-1)$ times.  Each of these calls of \texttt{min} causes an evaluation of the comparison
operator ``$\preceq$''.  If the number of evaluations of the comparison operator used to sort a list
$l$ of length $n$ is written as $b_n$, we have \\[0.2cm]
\hspace*{1.3cm}
$b_0 = 0$ \quad und \quad $b_{n+1} = b_n + n$. 
\\[0.2cm]
The reasoning is as follows: In order to sort a list of $n+1$ elements using selection sort we first
have to compute the minimum of this list.  We need $n$ comparisons for this.  Next, the minimum is
removed from the list and the remaining list, which only contains $n$ elements, is sorted
recursively.  We need $b_n$ evaluations of the comparisons operator for this recursive invocation of
\texttt{sort}.

When investigating the complexity of \emph{insertion sort} we had arrived at the same recurrence
relation. We had found the solution of this recurrence relation to be
\\[0.2cm]
\hspace*{1.3cm} $b_n = \frac{1}{2} \cdot n^2 - \frac{1}{2}\cdot n = \frac{1}{2} \cdot n^2 +
\Oh(n)$. 
\\[0.2cm]
It seems that the number of comparisons done by \emph{insertion sort} is the same as the number of
comparisons needed for \emph{selection sort}.  However, let us not jump to conclusions.
The algorithm \emph{insertion sort} needs
$\frac{1}{2}\cdot n \cdot (n-1)$ comparisons in the worst case while \emph{selection sort}
\underline{alwa}y$\!\!$\underline{$\;$s} uses $\frac{1}{2} \cdot n\cdot(n-1)$ comparisons.
In order to compute the minimum of a list of length $n$ we always have to do $n-1$ comparisons.
However, in order to insert an element into a list of $n$ elements, we only expect to do about
$\frac{1}{2} \cdot n$ comparisons on average.  The argument is that we expect about half the elements  to
be less than the element to be inserted.  Therefore, the average number of comparisons used by
insertion sort is only
\\[0.2cm]
\hspace*{1.3cm}
 $\frac{1}{4} \cdot n^2 + \Oh(n)$
\\[0.2cm]
and this is half as much as the number of comparisons used by \emph{selection sort}.  Therefore, in
general we expect \emph{selection sort} to need about twice as many comparisons as \emph{insertion sort}.



\section{Merge Sort}
Next, we discuss \href{http://en.wikipedia.org/wiki/Merge_sort}{\emph{merge sort}}.  This algorithm
is the first \underline{efficient} sorting algorithm that we encounter on our journey into the
wonderland of algorithms: We will see that \emph{merge sort} 
only needs $\Oh\bigl(n \cdot \log_2(n)\bigr)$ comparisons to sort a list of $n$ elements.  
The \emph{merge sort} algorithm was discovered by
\href{http://en.wikipedia.org/wiki/John_von_Neumann}{John von Neumann} in 1945.  John von Neumann 
was one of the most prominent mathematicians of the last century. 

In order to sort a list $l$ the algorithm proceeds as follows:
\begin{enumerate}
\item If $l$ has less than two elements, then $l$ is already sorted.  Therefore we have: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\#l < 2 \rightarrow \mathtt{sort}(l) = l$.
\item Otherwise, the list $l$ is split into two lists that have approximately the same size.
      These lists are then sorted recursively.  Then, the sorted lists are merged in a way that the
      resulting list is sorted: \\[0.2cm]
      \hspace*{1.3cm} 
      $\#l \geq 2 \rightarrow \mathtt{sort}(l) = \mathtt{merge}\bigl(\mathtt{sort}\bigl(\mathtt{split}_1(l)\bigr), \mathtt{sort}\bigl(\mathtt{split}_2(l)\bigr)\bigr)$
     \\[0.2cm]
     Here, $\texttt{split}_1$ and $\mathtt{split}_2$ are functions that split up the list $l$ into
     two parts, while the function \texttt{merge} takes two sorted lists and combines their element
     in a way that the resulting list is sorted.
\end{enumerate}
Figure \ref{fig:merge-sort.stlx} shows how these equations can be implemented as a \textsc{SetlX}
program.  The two functions $\mathtt{split}_1$ and $\mathtt{split}_2$ have been combined into one
function \texttt{split} that returns a pair of lists, i.~e.~the expression
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{split}(l)$
\\[0.2cm]
returns a pair of lists of the form
\\[0.2cm]
\hspace*{1.3cm}
$[l_1, l_2]$.
\\[0.2cm]
The idea is to distribute the elements of $l$ to the lists $l_1$ and $l_2$ in a way that both lists
have approximately the same size.   Let us proceed to discuss the details of the program shown in
Figure \ref{fig:merge-sort.stlx}:


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(l) {
        if (#l < 2) {
            return l;
        }
        [ l1, l2 ] := split(l);
        return merge(sort(l1), sort(l2));
    };    
    split := procedure(l) {
        match (l) {
            case []      : return [ [], [] ];
            case [x]     : return [ [x], [] ];
            case [x,y|r] : [r1,r2] := split(r);
                           return [ [x] + r1, [y] + r2 ];
        }
    };
    merge := procedure(l1, l2) {
        match ([l1, l2]) {
            case [ [], l2 ]        : return l2;
            case [ l1, [] ]        : return l1;
            case [ [x|r1], [y|r2] ]: if (x < y) {
                                         return [x] + merge(r1, l2);
                                     } else {
                                         return [y] + merge(l1, r2);
                                     }
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{The \emph{merge sort} algorithm implemented in \textsc{SetlX}.}
  \label{fig:merge-sort.stlx}
\end{figure}
\begin{enumerate}
\item If the list $l$ has less than two elements, it is already sorted and, therefore, it
      can be returned as it is.
\item The call to \texttt{split} distributes the elements of $l$ to the lists \texttt{l1} and \texttt{l2}.
\item These lists are sorted recursively and the resulting sorted lists are then merged.
\end{enumerate}

\noindent
Next, we specify the function $\texttt{split}$ via equations.
\begin{enumerate}
\item If $l$ is empty, $\mathtt{split}(l)$ returns two empty lists:\\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{split}(\texttt{[]}) = \mathtt{[} \texttt{[]}, \texttt{[]} \mathtt{]}$.
\item If $l$ contains exactly one element, this element is put into the first of the two lists
      returned from \texttt{split}: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{split}(\mathtt{[}x\mathtt{]}) = \mathtt{[} \texttt{[}x\texttt{]}, \texttt{[]} \mathtt{]}$.
\item Otherwise, $l$ must have the form $\mathtt{[}x, y\mathtt{]} + r$.
      Then we split $r$ recursively into two lists $r_1$ and $r_2$.  The element $x$ is put in front
      of $r_1$, while $y$ is put in front of $r_2$:
      \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{split}(r) = \mathtt{[}r_1, r_2\mathtt{]} \rightarrow
      \mathtt{split}\bigl(\mathtt{[}x, y\mathtt{]} + r\bigr) = \bigl[ [x] + r_1, [y] + r_2 \bigr]$.
\end{enumerate}
Finally, we specify how two sorted lists $l_1$ and $l_2$ are merged in a way that the resulting list
is also sorted.
\begin{enumerate}
\item If the list $l_1$ is empty, the result is $l_2$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{merge}([], l_2) = l_2$.
\item If the list $l_2$  is empty, the result is $l_1$: \\[0.2cm]
      \hspace*{1.3cm} 
      $\mathtt{merge}(l_1, []) = l_1$.
\item Otherwise, $l_1$ must have the form $[x] + r_1$ and $l_2$ has the form $[y] + r_2$.
      Then there is a case distinction with respect to the comparison of $x$ and $y$:
      \begin{enumerate}
      \item $x \preceq y$.

            In this case, we merge $r_1$ and $l_2$ and put $x$ at the beginning of this list:\\[0.2cm]
            \hspace*{1.3cm} 
            $x \preceq y \rightarrow \mathtt{merge}\bigl([x]+r_1, [y]+r_2\bigr) = [x] + \mathtt{merge}\bigl(r_1,[y]+r_2\bigr)$.
      \item $\neg x \preceq y$.

            Now we merge $l_1$ and $r_2$ and put $y$ at the beginning of this list:\\[0.2cm]
            \hspace*{1.3cm} 
            $\neg x \preceq y \rightarrow \mathtt{merge}\bigl([x]+r_1, [y]+r_2\bigr) = [y] + \mathtt{merge}\bigl([x] + r_1,r_2\bigr)$.
      \end{enumerate}
\end{enumerate}

\subsection{Complexity of Merge Sort}
Next, we compute the number of comparisons that are needed to sort a list of $n$
elements via merge sort.  To this end, we first analyze the number of comparisons that 
are done in a call of $\mathtt{merge}(l_1, l_2)$.   In order to do this we define the function \\[0.2cm]
\hspace*{1.3cm} 
$\mathtt{cmpCount}: \textsl{List}(m) \times \textsl{List}(m) \rightarrow \mathbb{N}$ 
\\[0.2cm]
such that, given two lists $l_1$ and $l_2$ of elements of some set $m$ the expression $\mathtt{cmpCount}(l_1, l_2)$ returns the
number of comparisons needed to compute $\texttt{merge}(l_1,l_2)$. 
Our claim is that, for any lists $l_1$ and $l_2$ we have  
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{cmpCount}(l_1, l_2) \leq \# l_1 + \# l_2$. 
\\[0.2cm]
The proof is done by induction on $\#l_1 + \#l_2$.
\begin{enumerate}
\item[I.A.:] $\#l_1 + \#l_2=0$.

             Then both $l_1$ and $l_2$ are empty and therefore the evaluation of
             $\mathtt{merge}(l_1, l_2)$ does not need any comparisons.  Therefore, we have 
             \\[0.2cm]
             \hspace*{1.3cm}
             $\mathtt{cmpCount}(l_1, l_2) = 0 \leq 0 = \#l_1 + \#l_2$.
\item[I.S.:] $\#l_1 + \#l_2 = n+1$.

             If either $l_1$ or $l_2$ is empty, then we do not need any comparisons in order to
             compute $\mathtt{merge}(l_1, l_2)$ and, therefore, we have
             \\[0.2cm]
             \hspace*{1.3cm}
             $\mathtt{cmpCount}(l_1,l_2) = 0 \leq \#l_1 + \#l_2$.
             \\[0.2cm]
             Next, let us assume that \\[0.2cm]
             \hspace*{1.3cm} $l_1 = [x] + r_1$ \quad and \quad $l_2 = [y] + r_2$.
             \\[0.2cm]
             We have to do a case distinction with respect to the relative size of $x$ and $y$.
             \begin{enumerate}
             \item $x \preceq y$.  Then we have \\[0.2cm]
                   \hspace*{1.3cm} 
                   $\mathtt{merge}\bigl([x] + r_1, [y] + r_2\bigr) = [x] + \mathtt{merge}\bigl(r_1, [y] + r_2\bigr)$. 
                   \\[0.2cm]
                   Therefore, we have \\
                   $\mathtt{cmpCount}(l_1, l_2) = 1 + \mathtt{cmpCount}(r_1, l_2) \stackrel{IH}{\leq} 1 + \#r_1 + \#l_2 = \#l_1 + \#l_2$.
             \item $\neg x \preceq y$.  This case is similar to the previous case. 
                   \qed
             \end{enumerate}
\end{enumerate}
\exercise
What is the form of the lists $l_1$ and $l_2$ that maximizes the value of 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{cmpCount}(l_1, l_2)$?
\\[0.2cm]
What is the value of $\mathtt{cmpCount}(l_1, l_2)$ in this case? \eox
\vspace*{0.3cm}

\noindent
Now we are ready to compute the complexity of \emph{merge sort} in the worst case.  Let us denote
the number of comparisons needed to sort a list $l$ of length $n$ as $f(n)$.  The algorithm \emph{merge sort} 
splits the list $l$ into two lists of length $l\symbol{92}2$, then sorts these lists recursively,
and finally merges the sorted lists.  Merging two lists of length $n \symbol{92}2$ can be done with
at most $n$ comparisons.  Therefore, the function $f$ satisfies the recurrence relation
\\[0.2cm]
\hspace*{1.3cm}
$f(n) = 2 \cdot f(n \symbol{92} 2) + \Oh(n)$,
\\[0.2cm]
According to the master theorem we therefore have
\\[0.2cm]
\hspace*{1.3cm}
$f(n) \in \Oh\bigl(n \cdot \log_2(n) \bigr)$.
\\[0.2cm]
This result already shows that, for large inputs, \emph{merge sort} is considerably more efficient
than both \emph{insertion sort} and \emph{selection sort}.  However, if we want to compare
\emph{merge sort} with \emph{quick sort}, the result $f(n) \in \Oh\bigl(n \cdot \log_2(n) \bigr)$ is
not precise enough.  In order to arrive at a bound for the number of comparisons that is more precise,
we need to solve the recurrence equation given above.  In order to simplify things,  define $a_n := f(n)$ 
and assume that $n$ is a power of $2$, i.~e.~we assume that
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle n = 2^k$ \qquad for some $k \el \mathbb{N}$.
\\[0.2cm]
Let us define $b_k := a_n = a_{2^k}$.  First, we compute the initial value $b_0$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle b_0 = a_{2^0} = a_1 = 0$,
\\[0.2cm]
since we do not need any comparisons when sorting a list of length one.  Since merging two lists of
length $2^k$ needs at most  $2^k + 2^k = 2^{k+1}$ comparisons, $b_{k+1}$ can be upper bounded as follows:
\\[0.2cm]
\hspace*{1.3cm} $b_{k+1} = 2 \cdot b_k + 2^{k+1}$ \hspace*{\fill} (1) \\[0.2cm]
In order to solve this recurrence equation, we substitute $k+1$ for $k$ in equation (1).  Then we have
\\[0.2cm]
\hspace*{1.3cm} 
$b_{k+2} = 2 \cdot b_{k+1} + 2^{k+2}$. \hspace*{\fill} (2) 
\\[0.2cm]
If we multiply equation (1) with $2$ and subtract the resulting equation from equation (2) we get
\\[0.2cm]
\hspace*{1.3cm} 
$b_{k+2} - 2 \cdot b_{k+1} = 2 \cdot b_{k+1} - 4 \cdot b_k$. 
\hspace*{\fill} (3) \\[0.2cm]
This equation can be rearranged as \\[0.2cm]
\hspace*{1.3cm}
$b_{k+2} = 4 \cdot b_{k+1} - 4 \cdot b_k$. \hspace*{\fill} (4) 
\\[0.2cm]
Equation (4) is a homogenous recurrence relation of second order.
It can be solved using the 
{\emph{approach}}
\\[0.2cm]
\hspace*{1.3cm}
$b_k = \lambda^k$.
\\[0.2cm]
This approach leads to the following quadratic equation for $\lambda$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{ll}
                & \lambda^2 = 4 \cdot\lambda - 4      \\[0.1cm]
\Leftrightarrow & \lambda^2 - 4 \cdot\lambda + 4 = 0  \\[0.1cm]
\Leftrightarrow & (\lambda-2)^2 = 0                   \\[0.1cm]
\Leftrightarrow & \lambda = 2
\end{array}
$ 
\\[0.2cm]
In general, a quadratic equation has two solutions. Since we have found only one solution, the
general solution of the recurrence relation is given as
\\[0.2cm]
\hspace*{1.3cm} 
$\displaystyle b_k = \alpha \cdot 2^k + \beta \cdot k \cdot 2^k$. \hspace*{\fill} (5) 
\\[0.2cm]
Substituting $k \mapsto 0$ we immediately see that \\[0.2cm]
\hspace*{1.3cm}
$0 = \alpha$. 
\\[0.2cm]
Substituting $k \mapsto 1$ in equation (1) gives
\\[0.2cm]
\hspace*{1.3cm}
 $b_1 = 2 \cdot b_0 + 2^1 = 2$.  
\\[0.2cm]
Substituting this value into equation (5) for $k=1$ shows that we must have \\[0.2cm]
\hspace*{1.3cm}
 $2 = 0 \cdot 2^1 + \beta \cdot 1 \cdot 2^1$.  \\[0.2cm]
Therefore we have $\beta = 1$.  Hence, the solution of the recurrence relation (1) is 
\\[0.2cm]
\hspace*{1.3cm}
$b_k = k \cdot 2^k$. 
\\[0.2cm]
Since $n = 2^k$ implies $k = \log_2(n)$ we have found
\\[0.2cm]
\hspace*{1.3cm}
$a_n = n \cdot \log_2(n)$. 


\subsection{Implementing Merge Sort for Arrays}
All the implementations of the \textsc{SetlX} programs presented up to now are quite inefficient.  The
reason is that, in \textsc{SetlX}, lists are internally represented as arrays.  Therefore, when
we evaluate an expression of the form 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[ x ] + r}
\\[0.2cm]
the following happens:
\begin{enumerate}
\item A new array is allocated.  This array will later hold the resulting list.
\item The element \texttt{x} is copied to the beginning of this array.
\item The elements of the list \texttt{r} are copied to the positions following \texttt{x}.
\end{enumerate}
Therefore, evaluating \texttt{[x] + r} for a list \texttt{r} of length $n$ requires $\Oh(n)$ data
movements.  Hence the \textsc{SetlX} programs given up to now are very inefficient.  In order to arrive at an
implementation that is more efficient we need to make use of the fact that lists are represented as arrays.
Figure \ref{fig:merge-sort-array.stlx} on page \pageref{fig:merge-sort-array.stlx} presents
an implementation of \emph{merge sort} that treats the list $l$ that is to be sorted as an array.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.4cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(rw l) {
        a := l;
        mergeSort(l, 1, #l + 1, a); 
    };
    mergeSort := procedure(rw l, start, end, rw a) {
        if (end - start < 2) { return; }
        middle := (start + end) \ 2;
        mergeSort(l, start,  middle, a);  
        mergeSort(l, middle, end   , a);    
        merge(l, start, middle, end, a); 
    };
    merge := procedure(rw l, start, middle, end, rw a) {    
        for (i in [start .. end-1]) { a[i] := l[i]; }
        idx1 := start;
        idx2 := middle;
        i    := start;
        while (idx1 < middle && idx2 < end) {
            if (a[idx1] <= a[idx2]) {
                l[i] := a[idx1]; i += 1; idx1 += 1;
            } else {
                l[i] := a[idx2]; i += 1; idx2 += 1;
            }
        }
        while (idx1 < middle) { l[i] := a[idx1]; i += 1; idx1 += 1; }
        while (idx2 < end   ) { l[i] := a[idx2]; i += 1; idx2 += 1; }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{An array based implementation of \emph{merge sort}.}
  \label{fig:merge-sort-array.stlx}
\end{figure}
We discuss the implementation shown in Figure \ref{fig:merge-sort-array.stlx} line by line.
\begin{enumerate}
\item In line 1 the keyword ``\texttt{rw}'' specifies that the parameter $l$ is a
      \emph{\underline{r}ead-\underline{w}rite} parameter.  Therefore, changes to $l$ remain
      visible after $\texttt{sort}(l)$ has returned.  This is also the reason that the procedure 
      \texttt{sort} does not return a result.  Instead, the evaluation of the expression
      $\mathtt{sort}(l)$ has the side effect of sorting the list $l$.
\item The purpose of the assignment ``\texttt{a := l;}'' in line 2 is to create an auxiliary array
      \texttt{a}.  This auxiliary array is needed in the procedure
      \texttt{mergeSort} called in line 3.
\item The procedure \texttt{mergeSort} defined in line 5 is called with 4 arguments.
      \begin{enumerate}
      \item The first parameter \texttt{l} is the list that is to be sorted.
      \item However, the task of \texttt{mergeSort} is not to sort all of \texttt{l} but only
            the part of \texttt{l} that is given as
            \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{l[start .. end-1]}. 
            \\[0.2cm]
            Hence, the parameters \texttt{start} and \texttt{end} are indices specifying the 
            subarray that needs to be sorted.
      \item The final parameter \texttt{a} is used as an auxiliary array.  This array is needed
            as temporary storage and it needs to have the same size as the list \texttt{l}.
      \end{enumerate} 
\item Line 6 deals with the case that the sublist of \texttt{l} that needs to be sorted is of length
      less than two.  In this case, there is nothing to do as any list of this length is already sorted.
\item One advantage of interpreting the list \texttt{l} as an array is that we do no longer
      need to implement a function \texttt{split} that splits the list \texttt{l} into two parts. 
      Instead, in line 7 we compute the index pointing to the middle element of the list \texttt{l} using the
      formula \\[0.2cm]
      \hspace*{1.3cm} 
      \texttt{middle = (start + end) \symbol{92} 2;} 
      \\[0.2cm]
      This way, the list \texttt{l} is split into the lists 
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{l[start .. middle-1]} \quad and \quad \texttt{l[middle .. end-1]}.
      \\[0.2cm]
      These two lists have approximately the same size which is half the size of the list \texttt{l}.
\item Next, the lists \texttt{l[start..middle-1]} and \texttt{l[middle..end-1]} are sorted
      recursively in line 8 and 9, respectively.
\item The call to \texttt{merge} in line 10 merges these lists.
\item The procedure \texttt{merge} defined in line 12 has 5 parameters: 
      \begin{enumerate}
      \item The first parameter \texttt{l} is the list that contains the two sublists that have to be merged.
      \item The parameters \texttt{start}, \texttt{middle}, and \texttt{end} specify the sublists
            that have to be merged.  The first sublist is 
            \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{l[start .. middle-1]}, 
            \\[0.2cm]
            while the second sublist is \\[0.2cm]
            \hspace*{1.3cm} 
            \texttt{l[middle .. end-1]}. 
      \item The final parameter $a$ is used as an auxiliary array.  It needs to be a list of the
            same size as the list \texttt{l}.
      \end{enumerate}
\item The function \texttt{merge} assumes that the sublists \texttt{l[start..middle-1]} and \linebreak
      \texttt{l[middle..end-1]} are already sorted.  The merging of these sublists works as follows:
      \begin{enumerate}
      \item First, line 13 copies the sublists into the auxiliary array \texttt{a}.
      \item In order to merge the two sublists stored in \texttt{a} into the list \texttt{l} we define
            three indices: 
            \begin{itemize}
            \item \texttt{idx1} points to the next element of the first sublist stored in \texttt{a}.
            \item \texttt{idx2} points to the next element of the second sublist stored in \texttt{a}.
            \item \texttt{i} points to the position in the list \texttt{l} where we have to put the next
                       element.
            \end{itemize}
      \item As long as neither the first nor the second sublist stored in \texttt{a} have been exhausted
            we compare in line 17 the elements from these sublists and then copy the smaller of these
            two elements into the list \texttt{l} at position \texttt{i}.
            In order to remove this element from the corresponding sublist in \texttt{a} we just need to
            increment the corresponding index pointing to the beginning of this sublist.
      \item If one of the two sublists gets empty while the other sublist still has elements, then we have
            to copy the remaining elements of the non-empty sublist into the list \texttt{l}.
            The \texttt{while}-loop in line 24 covers the case that the second sublist is exhausted before 
            the first sublist, while the \texttt{while}-loop in line 25 covers the case that the first
            sublist is exhausted before the second sublist.
      \end{enumerate}
\end{enumerate}

\subsection{An Iterative Implementation of Merge Sort}

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(rw l) {
        a := l;
        mergeSort(l, a);
    };    
    mergeSort := procedure(rw l, rw a) {
        n := 1;
        while (n < #l) {
            k := 0;
            while (n * (k + 1) + 1 <= #l) {
                merge(l, n*k+1, n*(k+1)+1, min([n*(k+2),#l])+1, a);
                k += 2;    
            }
            n *= 2;
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{A non-recursive implementation of \emph{merge sort}.}
  \label{fig:merge-sort-nr.stlx}
\end{figure}

\noindent
The implementation of \emph{merge sort} shown in Figure \ref{fig:merge-sort-array.stlx} on page
\pageref{fig:merge-sort-array.stlx} is recursive.  Unfortunately, the efficiency of a recursive
implementation of \emph{merge sort} is suboptimal.  The reason is that function calls are quite
costly since the arguments of the function have to be placed on a stack.  As a recursive
implementation has lots of function calls, it is less efficient than an iterative implementation.
Therefore, we present an iterative implementation of \emph{merge sort} in Figure
\ref{fig:merge-sort-nr.stlx} on page \pageref{fig:merge-sort-nr.stlx}.

Instad of recursive calls of the function \texttt{mergeSort}, this implementation has two nested 
\texttt{while}-loops.  The idea is to first split the list \texttt{l} into sublists of length 1.
Obviously, these sublists are already sorted.  Next, we merge these lists in pairs into lists of
length 2.  After that, we take pairs of lists of length 2 and merge them into sorted lists of length
4. Proceeding in this way we generate sorted lists of length
$8$, $16$, $\cdots$.  This algorithm only stops when \texttt{l} itself is sorted.

The precise working of this implementation gets obvious if we formulate the invariants of the
\texttt{while}-loops.  The invariant of the outer loop states that all sublists of \texttt{l} 
that have the form
\\[0.2cm]
\hspace*{1.3cm}
\texttt{l[n*k+1 .. n*(k+1)]}
\\[0.2cm]
are already sorted.  It is the task of the outer while loop to build pairs of sublists of this kind
and to merge them into a sublist of length $2 \cdot n$.

In the expression $l[n \cdot k + 1, \cdots, n \cdot (k+1)]$ the variable $k$ denotes a natural
number that is used to numerate the sublists.  The index $k$ of the first sublists is $0$ and
therefore this sublists has the form
\\[0.2cm]
\hspace*{1.3cm}
\texttt{l[1 .. n]},
\\[0.2cm]
while the second sublist is given as
\\[0.2cm]
\hspace*{1.3cm}
\texttt{l[n+1 .. 2*n]}.
\\[0.2cm]
It is possible that the last sublist has a length that is less than $n$.  This happens if the length
of \texttt{l} is not a multiple of $n$.  Therefore, the third argument of the call to \texttt{merge}
in line 10 is the minimum of $n\cdot(k+2)$ and $\mathtt{\#}l$.

\subsection{Further Improvements of Merge Sort}
The implementation given above can still be improved in a number of ways.  
\href{http://c2.com/cgi/wiki?TimPeters}{Tim Peters} has used a number of tricks to improve the
practical performance of \emph{merge sort}.  The resulting algorithm is known as
\href{http://en.wikipedia.org/wiki/Timsort}{\emph{Timsort}}.

The starting point of the development of \emph{Timsort} was the observation that the input arrays 
given to a sorting procedure often contain subarrays that are already sorted, either ascendingly or
descendingly.   For this reason, \emph{Timsort} uses the following tricks:
\begin{enumerate}
\item First, Timsort looks for subarrays that are already sorted.
      If a subarray is sorted descendingly, this subarray is reversed.
\item Sorted subarrays that are too small (i.~e.~have less than 32 elements) are extended
      to sorted subarrays to have a length that is at least 32.  In order to sort these subarrays,
      \emph{insertion sort} is used.  The reason is that \emph{insertion sort} is very fast for
      arrays that are already partially sorted.  The version of \emph{insertion sort} that is used is called
      \emph{binary insertion sort} since it uses 
      \href{http://en.wikipedia.org/wiki/Binary_search}{\emph{binary search}} to insert the elements
      into the array.
\item The algorithm to merge two sorted lists can be improved by the following observation: If we
      want to merge the arrays
      \\[0.2cm]
      \hspace*{1.3cm}
      $[x] + r$ \quad and \quad $l_1 + [y] + l_2$
      \\[0.2cm]
      and if $y$ is less than $x$, then all elements of the list $l_1$ are also less than $x$.
      Therefore, there is no need to compare these elements with $x$ one by one.  
\end{enumerate}
Timsort uses some more tricks, but unfortunately we don't have the time to discuss all of them.
\emph{Timsort} is part of the \textsl{Java} library, the source code is available online at
\\[0.2cm]
\hspace*{1.3cm}
\href{http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/jdk7-b76/src/share/classes/java/util/TimSort.java}{\texttt{http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/jdk7-b76/\\
\hspace*{2.5cm}
    src/share/classes/java/util/TimSort.java}}
\\[0.2cm]
Timsort is also used on the Android platform.

\section{\emph{Quick Sort}}
In 1961, C.~A.~R. Hoare discovered the \emph{quick sort} algorithm \cite{hoare:61}.  The basic idea
is as follows:
\begin{enumerate}
\item If the list $l$ that is to be sorted is empty, we return $l$: 
      \\[0.2cm]
      \hspace*{1.3cm} $\mathtt{sort}([]) = []$.
\item Otherwise, we have $l = [x] + r$.  In this case, we split $r$ into two lists $s$ and $b$.
      The list $s$ ($s$ stands for \underline{s}mall) contains all the elements of $r$ that are less
      or equal than $x$,     while $b$ ($b$ stands for \underline{b}ig) contains
      those elements of $r$ that are bigger than $x$.  The computation of $s$ and $b$ is done by a
      function called $\mathtt{partition}$: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{partition}(x,r) = \langle s,b \rangle$.
      \\[0.2cm]
      Formally, the function $\texttt{partition}$ can be defined as follows:
      \begin{enumerate}
      \item $\textsl{partition}(x, []) = \bigl\langle[], []\bigr\rangle$,
      \item \quad 
            $\; y \preceq x\; \wedge \textsl{partition}(x,r) = \bigl\langle s, b \bigr\rangle \rightarrow 
             \textsl{partition}(x, [y] + r) = \bigl\langle [y] + s,\, b \bigr\rangle$,
      \item $\neg\, (y \preceq x) \wedge \textsl{partition}(x,r) = \bigl\langle s, b \bigr\rangle \rightarrow 
             \textsl{partition}(x, [y] + r) = \bigl\langle s,\, [y] + b \bigr\rangle$.
      \end{enumerate}
      After partitioning the list $l$ into $s$ and $b$, the lists $s$ and $r$ are sorted
      recursively.  Then, the result is computed by appending the lists $\mathtt{sort}(s)$,
      $[x]$, and $\mathtt{sort}(b)$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{partition}(x,r) = \langle s,b \rangle \rightarrow 
       \textsl{sort}([x] + r) = \textsl{sort}(s) + [x] + \textsl{sort}(b)$.
\end{enumerate}
Figure \ref{fig:quick-sort.stlx} on page \pageref{fig:quick-sort.stlx} shows how these equations can
be implemented in \textsc{SetlX}.

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(l) {
        match (l) {
            case []   : return [];
            case [x|r]: [s,b] := partition(x, r);
                        return sort(s) + [x] + sort(b);
        }
    };
    partition := procedure(p, l) {
        match (l) {
            case []   : return [ [], [] ];
            case [x|r]: [ r1, r2 ] := partition(p, r);
                        if (x <= p) {
                            return [ [x] + r1, r2 ];
                        } else {
                            return [ r1, [x] + r2 ];
                        }    
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{The \emph{quick sort} algorithm.}
  \label{fig:quick-sort.stlx}
\end{figure}

\subsection{Complexity}
Next, we investigate the computational complexity of \emph{quick sort}.
Our goal is to compute the number of comparisons that are needed when
$\texttt{sort}(l)$ is computed for a list $l$ of length $n$.  In order to compute this number we
first investigate how many comparisons are needed for evaluating
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{partition}(p,l)$ 
\\[0.2cm]
for a list $l$ of $n$ elements: Each of the $n$ elements of $l$ has to be compared with $x$.
Therefore,  we need $n$ comparisons to compute $\mathtt{partition}(p,l)$.  The number of comparisons
for evaluating $\mathtt{sort}(l)$ depends on the result of $\texttt{partition}(p,l)$.  There is a
best case and a worst case.  We investigate the worst case first.

\subsubsection{Worst Case Complexity}
Let us denote the number of comparisons needed to evaluate $\mathtt{sort}(l)$ for a list $l$ of
length $n$ in the worst case as $a_n$.  The worst case occurs if the call to \texttt{partition}
returns a pair of the form
\\[0.2cm]
\hspace*{1.3cm}
$\bigl[ [], b \bigr]$.
\\[0.2cm]
This happens if all elements of $l[2..]$ are bigger than $l[1]$.  Then, we have
\\[0.2cm]
\hspace*{1.3cm}
$a_n = a_{n-1} + n - 1$. 
\\[0.2cm]
The term $n-1$ is due to the $n-1$ comparisons needed for execution
$\texttt{partition}(x,r)$ in line 4 of \ref{fig:quick-sort.stlx} and the term $a_{n-1}$ is the
number of comparisons needed for the recursive evaluation of $\textsl{sort}(b)$.

The initial condition is $a_1 = 0$, since we do not need any comparisons to sort a list
containing only one element.
Hence the recurrence relation can be solved as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
  a_n & = & a_{n-1} + (n-1) \\
      & = & a_{n-2} + (n-2) + (n-1) \\
      & = & a_{n-3} + (n-3) + (n-2) + (n-1) \\
      & = & \vdots \\
      & = & a_{1} + 1 + 2 + \cdots  + (n-2) + (n-1) \\
      & = & 0 + 1 + 2 + \cdots  + (n-2) + (n-1) \\[0.2cm]
      & = & \sum\limits_{i=0}^{n-1} i  =  \frac{1}{2} \cdot n \cdot(n - 1) =
            \frac{1}{2} \cdot n^2 - \frac{1}{2} \cdot n \\[0.4cm]
      & \in & \Oh(n^2)
\end{array}
$
\\[0.2cm]
This shows that in the worst case, the number of comparisons is as big as it is in the worst case of 
\emph{insertion sort}.  The worst case occurs if we try to sort a list $l$ that is already sorted.


\subsubsection{Average Complexity}
By this time you probably wonder why the algorithm has been called \emph{quick sort} since, in the worst case,
it much slower as \emph{merge sort}.  The reason is that the \underline{avera}g\underline{e} number $d_n$ of 
comparisons needed to sort a list of $n$ elements is $\Oh\bigl(n \cdot \log_2(n)\bigr)$. We prove
this claim next.  Let us first note the following: If $l$ is a list of $n+1$ elements, then the number of elements of the 
list $s$ that is computed in line 4 of Figure \ref{fig:quick-sort.stlx} is a member of the set 
$\{0,1,2,\cdots,n\}$.  If the length of $s$ is $i$ and the length of $l$ is $n+1$, then the length of
$b$ is $n-i$.  Therefore, if $\#s = i$, then on average we need
\\[0.2cm]
\hspace*{1.3cm} $d_i + d_{n-i}$ \\[0.2cm]
comparisons to sort $s$ and $b$ recursively.  If we take the average over all possible values of
 $i = \#s$ then, since $i \el\{0,1,\cdots,n\}$, we get the following recurrence relation for $d_{n+1}$:
\\[0.2cm]
\hspace*{1.3cm}
$d_{n+1} = n + \bruch{1}{n+1} \cdot \sum\limits_{i=0}^n (d_i + d_{n-i}) $ \hspace*{\fill} (1) 
\\[0.2cm] 
Here, the term $n$ accounts for the number of comparisons needed to compute 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{partition(x, r)}.
\\[0.2cm]
In order to simplify the recurrence relation (1) we note that
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
 \sum\limits_{i=0}^n f(n-i) & = & f(n) + f(n-1) + \cdots + f(1) + f(0) \\[0.2cm]
                    & = & f(0) + f(1) + \cdots + f(n-1) + f(n) \\[0.2cm]
                    & = & \sum\limits_{i=0}^n f(i) 
\end{array}
$
\\[0.2cm]
holds for any function $f:\mathbb{N} \rightarrow \mathbb{N}$.
This observation can be used to simplify the recurrence relation (1) as follows:
\\[0.2cm]
\hspace*{1.3cm}
$d_{n+1} = n + \bruch{2}{n+1} \cdot \sum\limits_{i=0}^n d_i$. \hspace*{\fill} (2)
\\[0.2cm] 
In order to solve this recurrence relation we substitute $n \mapsto n+1$ and arrive at
\\[0.2cm]
\hspace*{1.3cm}
$d_{n+2} = n+1 + \bruch{2}{n+2} \cdot \sum\limits_{i=0}^{n+1} d_i$.  \hspace*{\fill} (3)
\\[0.2cm]
Next, we multiply equation (3) with $n+2$ and equation (2) with $n+1$.  This yields the equations
\pagebreak

\noindent
\hspace*{1.3cm}
$(n+2)\cdot d_{n+2} = (n+2)\cdot(n+1) + 2 \cdot \sum\limits_{i=0}^{n+1} d_i$,
\hspace*{\fill} (4) \\[0.2cm]
\hspace*{1.3cm}
$(n+1)\cdot d_{n+1}  =  (n+1)\cdot n + 2 \cdot \sum\limits_{i=0}^n d_i$.  
\hspace*{\fill} (5) \\[0.2cm]
Now we take the difference of  equation (4) and (5) and note that the summations cancel except for
the term $2\cdot d_{n+1}$.  We get
\\[0.2cm]
\hspace*{1.3cm}
$(n+2)\cdot d_{n+2} - (n+1)\cdot \displaystyle d_{n+1} = (n+2)\cdot(n+1) - (n+1)\cdot n+2 \cdot d_{n+1}$
\\[0.2cm] 
This equation can be simplified as
\\[0.2cm]
\hspace*{1.3cm}
$(n+2)\cdot d_{n+2} = (n+3)\cdot \displaystyle d_{n+1} + 2\cdot(n+1)$.
\\[0.2cm] 
Following a divine inspiration we divide this equation by $(n+2) \cdot(n+3)$ and
get 
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{1}{n+3} \cdot d_{n+2} = \bruch{1}{n+2}\cdot d_{n+1} + \bruch{2\cdot(n+1)}{(n+2)\cdot(n+3)}$.
\hspace*{\fill} (6) 
\\[0.2cm]
In order to proceed we compute the 
\href{http://en.wikipedia.org/wiki/Partial_fraction}{\emph{partial fraction decomposition}}
of the fraction
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{2\cdot(n+1)}{(n+2)\cdot(n+3)}$.
\\[0.2cm] 
In order to so, we use the ansatz
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{2\cdot(n+1)}{(n+2)\cdot(n+3)} = \bruch{\alpha}{n+2} + \bruch{\beta}{n+3}$.
\\[0.2cm] 
Multiplying this equation with $(n+2) \cdot (n+3)$ yields
\\[0.2cm]
\hspace*{1.3cm}
$ 2\cdot n + 2 = \alpha \cdot (n+3) + \beta \cdot (n+2)$.
\\[0.2cm]
This can be simplified as follows:
\\[0.2cm]
\hspace*{1.3cm}
$2\cdot n + 2 = (\alpha + \beta) \cdot n + 3 \cdot \alpha  + 2 \cdot \beta$.
\\[0.2cm]
Comparing the coefficients produces two equations:
\begin{eqnarray*}
  2 & = & \alpha + \beta \\
  2 & = & 3 \cdot \alpha + 2 \cdot \beta 
\end{eqnarray*}
If we subtract the second equation twice from the first equation we arrive at
 $\alpha = -2$.  Substituting this into the first equation gives $\beta = 4$.
Hence, the equation (6) can be written as
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{1}{n+3} \cdot d_{n+2} = \bruch{1}{n+2}\cdot d_{n+1} - \bruch{2}{n+2} + \bruch{4}{n+3}$.  
\\[0.2cm]  
In order to simplify this equation, let us define
\\[0.2cm]
\hspace*{1.3cm}
 $\displaystyle a_n = \frac{d_n}{n+1}$. 
\\[0.2cm] 
Then the last equation is simplified to
\\[0.2cm]
\hspace*{1.3cm}
$a_{n+2} = a_{n+1} - \bruch{2}{n+2} + \bruch{4}{n+3}$.
\\[0.2cm] 
Substituting $n \mapsto n-2$ simplifies this equation: 
\\[0.2cm]
\hspace*{1.3cm}
$a_{n} = a_{n-1} - \bruch{2}{n} + \bruch{4}{n+1}$,
\\[0.2cm] 
This equation can be rewritten as a sum.  Since $a_0 = \bruch{d_0}{1} = 0$
we have
\\[0.2cm]
\hspace*{1.3cm}
$a_{n} = 4 \cdot \sum\limits_{i=1}^n \bruch{1}{i+1} - 2 \cdot \sum\limits_{i=1}^n \bruch{1}{i}$.  
\\[0.2cm]
Let us simplify this sum:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
 a_{n} & = & \displaystyle 4 \cdot \sum_{i=1}^n \frac{1}{i+1} - 2 \cdot \sum_{i=1}^n \frac{1}{i} \\[0.5cm]
       & = & \displaystyle 4 \cdot \sum_{i=2}^{n+1} \frac{1}{i} - 2 \cdot \sum_{i=1}^n \frac{1}{i} \\[0.5cm]
       & = & \displaystyle 4 \cdot \frac{1}{n+1} - 4 \cdot \frac{1}{1} + 4 \cdot \sum_{i=1}^{n} \frac{1}{i} - 2 \cdot \sum_{i=1}^n \frac{1}{i} \\[0.5cm]
       & = & \displaystyle 4 \cdot \frac{1}{n+1} - 4 \cdot \frac{1}{1} + 2 \cdot \sum_{i=1}^{n} \frac{1}{i}  \\[0.5cm]
       & = & \displaystyle - \frac{4 \cdot n}{n+1}  + 2 \cdot \sum_{i=1}^{n} \frac{1}{i}  
\end{array}
$
\\[0.2cm]
In order to finalize our computation we have to compute an approximation for the sum
\\[0.2cm]
\hspace*{1.3cm}
$H_n = \sum\limits_{i=1}^{n}\bruch{1}{i}$.
\\[0.2cm] 
The number $H_n$ is known in mathematics as the $n$-th 
\href{http://en.wikipedia.org/wiki/Harmonic_number}{\emph{harmonic number}}.
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Euler} (1707 -- 1783) was able to prove
that the harmonic numbers can be approximated as
\\[0.2cm]
\hspace*{1.3cm}
$ H_n = \ln(n) + \Oh(1)$. 
\\[0.2cm]  
Therefore, we have found the following approximation for $a_n$:
\\[0.2cm]
\hspace*{1.3cm}
$a_n = - \bruch{4 \cdot n}{n+1}  + 2 \cdot \ln(n) + \Oh(1)$.
\\[0.2cm] 
Since we have $d_n = (n+1) \cdot a_{n}$ we can conclude that
\begin{eqnarray*}  
 d_n & = & -4 \cdot n + 2 \cdot(n+1) \cdot H_n \\
     & = & -4 \cdot n + 2 \cdot(n+1) \cdot \bigl(\ln(n) + \Oh(1)\bigr) \\
     & = & 2 \cdot n \cdot \ln(n) + \Oh(n)
\end{eqnarray*}
holds.  Let us compare this result with the number of comparisons needed for \emph{merge sort}.
We have seen previously that \emph{merge sort} needs
\\[0.2cm]
\hspace*{1.3cm} $n \cdot \log_2(n) + \Oh(n)$ \\[0.2cm]
comparisons in order to sort a list of $n$ elements.  Since we have $\ln(n) = \ln(2) \cdot
\log_2(n)$
we conclude that the average case of \emph{quick sort} needs
 \\[0.2cm]
\hspace*{1.3cm} $2 \cdot \ln(2) \cdot n \cdot \log_2(n)$ \\[0.2cm]
comparisons and hence \emph{quick sort} needs  $2 \cdot \ln(2) \approx 1.39$ times as many comparisons as
\emph{merge sort}.  


\subsection{Implementing \emph{Quick Sort} for Arrays}
Finally, we show how \emph{quick sort}  is implemented for arrays.  Figure
\ref{fig:quick-sort-array.stlx} on page \pageref{fig:quick-sort-array.stlx} shows this implementation. 

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm
                ]
    sort := procedure(rw l) {
        quickSort(1, #l, l);
    };
    quickSort := procedure(a, b, rw l) {
        if (b <= a) {
            return; // at most one element, nothing to do
        }
        m := partition(a, b, l);  // split index
        quickSort(a, m - 1, l);
        quickSort(m + 1, b, l);
    };
    partition := procedure(start, end, rw l) {
        pivot := l[end];
        left  := start - 1;
        for (idx in [start .. end-1]) {
            if (l[idx] <= pivot) {
                left += 1;
                swap(left, idx, l);
            }
        }
        swap(left + 1, end, l);
        return left + 1;
    };    
    swap := procedure(x, y, rw l) {
        [ l[x], l[y] ] := [ l[y], l[x] ];
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{An implementation of \emph{quick sort} based on arrays.}
  \label{fig:quick-sort-array.stlx}
\end{figure}

\begin{enumerate}
\item Contrary to the array based implementation of \emph{merge sort}, we do not need an auxiliary
      array.  This is one of the main advantages of \emph{quick sort} over \emph{merge sort}.
\item The function \texttt{sort} is reduced to a call of \texttt{quickSort}.  This function
      takes the parameters \texttt{a}, \texttt{b}, and \texttt{l}.  
      \begin{enumerate}
      \item \texttt{a} specifies the index of the first element of the subarray that needs to be
            sorted.
      \item \texttt{b} specifies the index of the last element of the subarray that needs to be
            sorted. 
      \item \texttt{l} is the array that needs to be sorted.
      \end{enumerate}
      Calling \texttt{quickSort(a, b, l)} sorts the subarray \\[0.2cm]
      \hspace*{1.3cm} 
      \texttt{l[a], l[a+1], $\cdots$, l[b]}
      \\[0.2cm]
      of the array \texttt{l}, i.~e.~after that call we expect to have\\[0.2cm]
      \hspace*{1.3cm}
      \texttt{l[a] $\preceq$ l[a+1] $\preceq$ $\cdots$ $\preceq$ l[b]}.
      \\[0.2cm]
      The implementation of the function \texttt{quickSort}
      is quite similar to the list implementation.  The main difference is that the function
      \texttt{partition} that is called in line 8 redistributes the elements of \texttt{l} such that
      afterwards all elements that are less or equal than the \emph{pivot element} \texttt{l[m]}
      have an index that is lower than the index \texttt{m}, while the remaining elements will have
      an index that is bigger than \texttt{m}.  The pivot element itself will have the index \texttt{m}.
\item The difficult part of the implementation of \emph{quick sort} is the implementation of the
      function \texttt{partition} that is shown beginning in line 12.
      The \texttt{for} loop in line 15 satisfies the following invariants.
      \begin{enumerate}
      \item $\forall i \in \{ \mathtt{start}, \cdots, \mathtt{left} \} : \mathtt{l}[i] \leq \mathtt{pivot}$.

            All elements in the subarray \texttt{l[start..left]} are less or equal than the pivot element.
      \item $\forall i \in \{ \mathtt{left}+1,\cdots,\mathtt{idx}-1\}:\mathtt{pivot} < l[i]$.

            All elements in the subarray \texttt{l[left+1..idx-1]} are greater than the pivot
            element.
      \item $\texttt{pivot} = \mathtt{l[end]}$

            The pivot element itself is at the end of the array.
      \end{enumerate}
      Observe how these invariants are maintained.  If the element \texttt{l[idx]} is less than the
      pivot element, it need to become part of the subarray \texttt{l[start..left]}.  In order to
      achieve this, it is placed at the position \texttt{l[left+1]}.  The element that has been at
      that position is
      part of the subarray \texttt{l[left+1..idx-1]} and therefore, most of the times,\footnote{It is not always greater than the pivot element
      because the subarray \texttt{l[left+1..idx-1]} might well be empty.}
      it is greater than the pivot element.  
      Hence we move this element to the end of the subarray \texttt{l[left+1..idx-1]}.

      Once the \texttt{for} loop in line 15 terminates, the call to \texttt{swap} in line 21 moves
      the pivot element into its correct position.  
\end{enumerate}


\subsection{Improvements for \emph{Quick Sort}}
There are a number of tricks that can be used to increase the efficiency of \emph{quick sort}.
\begin{enumerate}
\item Instead of taking the first element as the pivot element, use three elements from the list
      $l$ that is to be sorted.  For example, take the first element, the last element, and an
      element from the middle of the list.  Now compare these three elements and take that element as
      a pivot that is between the other two elements.

      The advantage of this strategy is that  worst case performance is much more unlikely to occur.  In
      particular,  using this strategy the worst case won't occur for a list that is already
      sorted.
\item If a sublist contains fewer than 10 elements, use \emph{insertion sort} to sort this sublist.

      The paper ``\emph{Engineering a Sort Function}'' by Jon L.~Bentley and M.~Douglas McIlroy
      \cite{bentley:93} describes the previous two improvements.
\item In order to be sure that the average case analysis of \emph{quick sort} holds we can randomly
      \emph{shuffle} the list $l$ that is to be sorted.  In \textsc{SetlX} this is quite easy as
      there is a predefined function \texttt{shuffle} that takes a list and shuffles it randomly.
      For example, the expression
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{shuffle([1..10]);}
      \\[0.2cm]
      might return the result
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{[1, 9, 8, 5, 2, 10, 6, 3, 4, 7]}.
\item In 2009, Vladimir Yaroslavskiy introduced \emph{dual pivot quick sort} \cite{yaroslavskiy:2009}.  His paper can be
      downloaded at the following address:
      \\[0.2cm]
      \hspace*{0.3cm}
      \href{http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf}{\texttt{http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf}}
      \\[0.2cm]
      The main idea of Yaroslavskiy is to use two pivot elements $x$ and $y$.  For example, we can
      define
      \\[0.2cm]
      \hspace*{1.3cm}
      $x := l[1]$ \quad and \quad $y := l[\#l]$,
      \\[0.2cm]
      i.~e.~we take $x$ as the first element of $l$, while $y$ is the last element of $l$.  Then, the list
      $l$ is split into three parts:
      \begin{enumerate}
      \item The first part contains those elements that are less than $x$.
      \item The second part contains those elements that are bigger or equal than $x$ but less or
            equal than $y$.
      \item The third part contains those elements that are bigger than $y$.
      \end{enumerate}
      Figure \ref{fig:dual-pivot-quick-sort.stlx} on page \pageref{fig:dual-pivot-quick-sort.stlx}
      shows a simple list based implementation of \emph{dual pivot quick sort}.



      Various studies have shown that \emph{dual pivot quick sort} is faster than any other sorting
      algorithm.  For this reason, the version 1.7 of \textsl{Java} uses \emph{dual pivot quick sort}:
      \\[0.2cm]
      \hspace*{0.3cm}
      \href{http://www.docjar.com/html/api/java/util/DualPivotQuicksort.java.html}{http://www.docjar.com/html/api/java/util/DualPivotQuicksort.java.html} 
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    sort := procedure(l) {
        match (l) {
        case []     : return [];
        case [x]    : return [x];
        case [x,y|r]: [p1, p2] := [min({x,y}), max({x,y})];
                      [l1,l2,l3] := partition(p1, p2, r);
                      return sort(l1) + [p1] + sort(l2) + [p2] + sort(l3);
        }
    };
    partition := procedure(p1, p2, l) {
        match (l) {
        case []   : return [ [], [], [] ];
        case [x|r]: [ r1, r2, r3 ] := partition(p1, p2, r);
                    if (x < p1) {
                        return [ [x] + r1, r2, r3 ];
                    } else if (x <= p2) {
                        return [ r1, [x] + r2, r3 ];
                    } else {
                        return [ r1, r2, [x] + r3 ];
                    }
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A list based implementation of \emph{dual pivot quick sort}.}
\label{fig:dual-pivot-quick-sort.stlx}
\end{figure}

\exercise
Implement a version of \emph{dual pivot quick sort} that uses arrays instead of lists.


\section[A Lower Bound]{A Lower Bound for the Number of Comparisons Needed to Sort a List}
In this section we will show that any sorting algorithm that sorts elements by comparing them must
use at least 
\\[0.2cm]
\hspace*{1.3cm}
 $\Oh\bigl(n \cdot \ln(n)\bigr)$ 
\\[0.2cm]
comparisons.  The important caveat here is that the sorting algorithm is restricted to not make any assumptions
on the elements of the list $l$ that is to be sorted.  The only operation that is allowed on these
elements is the use of the comparison operator ``\texttt{<}''.  Furthermore, to simplify matters let
us assume that all elements of the list $l$ are distinct.

Let us consider lists of two elements first, i.~e.~assume we have
\\[0.2cm]
\hspace*{1.3cm}
$l = [a_1, a_2]$.  
\\[0.2cm]
In order to sort this list, one comparison is sufficient:
\begin{enumerate}
 \item If $a_1 < a_2$ then $[a_1, a_2]$ is sorted ascendingly.
 \item If $a_2 < a_1$ then $[a_2, a_1]$ is sorted ascendingly.
\end{enumerate}
If the list $l$ that is to be sorted has the form
\\[0.2cm]
\hspace*{1.3cm}
$l = [a_1,a_2,a_3]$ 
\\[0.2cm]
then there are 6 possibilities to arrange these elements:
\\[0.2cm]
\hspace*{0.3cm}
$[a_1,a_2,a_3]$, \quad
$[a_1,a_3,a_2]$, \quad
$[a_2,a_1,a_3]$, \quad
$[a_2,a_3,a_1]$, \quad
$[a_3,a_1,a_2]$, \quad
$[a_3,a_2,a_1]$. 
\\[0.2cm]
Now we need at least three comparisons, since with two comparisons we could at most choose between
four different possibilities.
In general there are 
\\[0.2cm]
\hspace*{1.3cm}
$n! = 1 \cdot 2 \cdot 3 \cdot {\dots} \cdot (n-1) \cdot n = \prod\limits_{i=1}^n i$ 
\\[0.2cm]
different permutations of a list of $n$ different elements. 
We prove this claim by induction. 
\begin{enumerate}
\item $n=1$:  

      There is only $1$ way to arrange one element in a list.  As $1 = 1!$ the claim ist proven
      in this case.
\item $n \mapsto n+1$:
  
      If we have $n+1$ different elements and want to arrange these elements in a list, then there
      are $n+1$ possibilities for the first element.  In each of these cases the induction
      hypotheses tells us that there are $n!$ ways to arrange the remaining $n$ elements in a list.
      Therefore, all in all there are $(n+1) \cdot n! = (n+1)!$ different arrangements of $n+1$
      elements in a list.
\end{enumerate}
Next, we consider how many different cases can be distinguished if we have $k$ different tests
that only give yes or no answers.  Tests of this kind are called \emph{binary tests}.
\begin{enumerate}
\item If we can only do only one binary test, then we can only distinguish between two cases.
\item If we have $2$ tests, then we can distinguish between  $2^2$ different cases.
\item In general, $k$ tests can choose from at most $2^k$ different cases.
\end{enumerate}
The last claim can be argued as follows:  If the results of the tests are represented as
$0$ and $1$, then $k$ binary tests correspond to a binary string of length
$k$.  However, binary strings of length $k$ can be used to code the numbers from $0$ up to
$2^{k}-1$.  We have
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{card}\bigl(\{0,1,2,\cdots, 2^k-1\}\bigr) = 2^k$.
\\[0.2cm]
Hence there are $2^k$ binary strings of length $k$.  

If we have a list of $n$ different elements then there are $n!$ different permutations of these
elements.  In order to figure out which of these $n!$ different permutations is given we have to
perform $k$ comparisons where we must have
\\[0.2cm]
\hspace*{1.3cm}
$2^k \geq n!$.
\\[0.2cm]
This immediately implies
\\[0.2cm]
\hspace*{1.3cm}
$k \geq \log_2(n!)$.
\\[0.2cm]
In order to proceed we need an approximation for the expression $\log_2(n!)$.  
A simple approximation of this term is
\\[0.2cm]
\hspace*{1.3cm}
$\log_2(n!) = n \cdot \log_2(n) + \Oh(n)$.
\\[0.2cm]
Using this approximation we get
\\[0.2cm]
\hspace*{1.3cm}
$k \geq n \cdot \log_2(n) + \Oh(n)$.
\\[0.2cm]
As \emph{merge sort} is able to sort a list of length $n$ using only $n \cdot \log_2(n)$ comparisons
we have now shown that this algorithm is optimal with respect to the number of comparisons.





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "algorithms"
%%% End: 
